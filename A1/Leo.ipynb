{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKkeYE20a37b"
   },
   "source": [
    "# COMP 579 Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itabypfB4KKI"
   },
   "source": [
    "1. Do Not Change the Random Seed\n",
    "The random seed has been set to ensure reproducibility. Please do not modify it.\n",
    "\n",
    "2. Guidance for the First Question\n",
    "For the initial question, fill in the blanks under the sections marked as TODO. Follow the provided structure and complete the missing parts.\n",
    "\n",
    "3. Approach for Subsequent Questions\n",
    "For the later questions, we expect you to attempt the solutions independently. You can refer to the examples provided in earlier questions to understand how to\n",
    "plot figures and implement solutions.\n",
    "\n",
    "4. Ensure that the plots you produce for later questions are similar in style and format to those shown in the previous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "eEvd8WcFqvai"
   },
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.special import softmax\n",
    "from IPython.core.debugger import set_trace\n",
    "np.random.seed(40)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"]=10,5"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ss77N5TbLVIl"
   },
   "source": [
    "## Q1 Simulator for Gaussian Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "id": "uG8suY4Sn7hu"
   },
   "source": [
    "\n",
    "class GaussianBandit:\n",
    "  \"\"\"\n",
    "    A class representing a Gaussian multi-armed bandit.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    num_arms : int\n",
    "        Number of arms in the bandit.\n",
    "    mean : list or np.ndarray\n",
    "        List of mean rewards for each arm.\n",
    "    variance : float\n",
    "        Variance of the rewards for all arms.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    sample(arm_index)\n",
    "        Samples a reward from the specified arm based on a Gaussian distribution.\n",
    "    \"\"\"\n",
    "\n",
    "  # TODO:\n",
    "  def __init__(self, num_arms, mean, variance):\n",
    "    self.num_arms = num_arms\n",
    "    self.mean = mean\n",
    "    self.variance = variance\n",
    "\n",
    "  def sample(self, arm_index):\n",
    "    reward = np.random.normal(loc=self.mean[arm_index], scale=np.sqrt(self.variance))\n",
    "    return reward\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "A2G0G_s5sy_C"
   },
   "source": [
    "# TODO:\n",
    "delta = 0.2\n",
    "num_arms = 3\n",
    "means = np.array([0.5, 0.5-delta, 0.5+delta])\n",
    "# means = [0.5, 0.5 - delta, 0.5 + delta]\n",
    "variance = 0.01\n",
    "num_samples = 50\n",
    "\n",
    "three_arm_gaussian_bandit = GaussianBandit(num_arms=num_arms, mean=means, variance=variance)\n",
    "\n",
    "# Store the rewards for each arm\n",
    "action_rewards = []\n",
    "actions = range(num_arms)\n",
    "\n",
    "for action in actions:\n",
    "    # Store 50 samples per action\n",
    "    rewards = []\n",
    "    for _ in range(num_samples):\n",
    "      rewards.append(three_arm_gaussian_bandit.sample(action))\n",
    "    action_rewards.append(rewards)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UG_coTYL1_RL"
   },
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pj04OZ0ozUjK",
    "outputId": "18faa6df-5163-4fad-8b88-d196fa53bbaa"
   },
   "source": [
    "for action in actions:\n",
    "  fig, ax = plt.subplots()\n",
    "\n",
    "  # TODO:\n",
    "  true_value = means[action]\n",
    "  estimated_value = np.mean(action_rewards[action])\n",
    "\n",
    "  # draw the line of the true value\n",
    "  line_true_val = ax.axhline(y = true_value, color = 'b', linestyle = ':', label = \"true value\")\n",
    "  # draw the line of the estimated value\n",
    "  line_est_val = ax.axhline(y = estimated_value, color = 'r', linestyle = '--', label = \"estimated value\")\n",
    "  # plot the reward samples\n",
    "  plt_samples, = ax.plot(action_rewards[action], 'o', label = \"reward samples\")\n",
    "\n",
    "  ax.set_xlabel(\"sample number\")\n",
    "  ax.set_ylabel(\"reward value\")\n",
    "  ax.set_title(\"Sample reward, estimated and true expected reward over 50 samples for action %s\" %action, y=-0.2)\n",
    "\n",
    "  # show the legend with the labels of the line\n",
    "  ax.legend(handles=[line_true_val, line_est_val, plt_samples])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKLg6U5bLhRs"
   },
   "source": [
    "## Q2 Estimated Q values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "id": "cgnKe19V0NgR"
   },
   "source": [
    "def update(reward_samples, alpha):\n",
    "  \"\"\"\n",
    "  Each call to the function yields the current incremental average of the reward with a fixed learning rate, alpha\n",
    "  E.g. Inital call returns alpha * reward_samples[0], second call returns prev_val + alpha * (reward_samples[1] - prev_val)\n",
    "  where prev_val is the value return from the previous call, so on and so forth\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  reward_samples : array of int\n",
    "      samples of reward values from one arm of a bandit\n",
    "  alpha : int\n",
    "      learning rate parameter for the averaging\n",
    "  \"\"\"\n",
    "  for i in range(len(reward_samples)):\n",
    "    if i == 0:\n",
    "      average = alpha*reward_samples[i]\n",
    "    else:\n",
    "      average = average + alpha*(reward_samples[i]-average)\n",
    "    yield average\n",
    "\n",
    "def updateAvg(reward_samples):\n",
    "\n",
    "  \"\"\"\n",
    "  Each call to the function yields the current incremental average of the reward\n",
    "  E.g. Inital call returns reward_samples[0], second call returns the average of reward_samples[0] and reward_samples[0], so on and so forth\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  reward_samples : array of int\n",
    "      samples of reward values from one arm of a bandit\n",
    "  \"\"\"\n",
    "\n",
    "  for i in range(len(reward_samples)):\n",
    "    if i == 0:\n",
    "      average = reward_samples[i]\n",
    "    else:\n",
    "      average = average + 1/(i+1)*(reward_samples[i]-average)\n",
    "    yield average\n",
    "\n",
    "def updateDecaying(reward_samples, alpha_0=0.5, lambda_=0.01, p=0.5):\n",
    "    \"\"\"\n",
    "    Each call to the function yields the updated estimate of the action value using an\n",
    "    improved decaying learning rate.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reward_samples : array-like of int or float\n",
    "        Samples of reward values from one arm of a bandit.\n",
    "    alpha_0 : float, optional\n",
    "        The initial learning rate (default is 0.5).\n",
    "    lambda_ : float, optional\n",
    "        The decay rate constant (default is 0.01).\n",
    "    p : float, optional\n",
    "        The power parameter for controlling decay (default is 0.5).\n",
    "    \"\"\"\n",
    "    for i in range(len(reward_samples)):\n",
    "      if i == 0:\n",
    "        average = reward_samples[i]*alpha_0\n",
    "      else:\n",
    "        alpha = alpha_0/((1+lambda_*(i+1))**p)\n",
    "        average = average + alpha*(reward_samples[i]-average)\n",
    "      yield average\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKmG74R11x-j"
   },
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "G_ltKRTcBaDM",
    "outputId": "8b30a980-51af-46c3-c0f9-1213f522ee60"
   },
   "source": [
    "for action in actions:\n",
    "  fig, ax = plt.subplots()\n",
    "\n",
    "  # TODO:\n",
    "  incr_avgs = list(updateAvg(action_rewards[action]))\n",
    "  alpha_1_percent = list(update(action_rewards[action], 0.01))\n",
    "  alpha_10_percent = list(update(action_rewards[action], 0.1))\n",
    "  alpha_decay = list(updateDecaying(action_rewards[action], alpha_0=0.5, lambda_=0.01, p=0.5))\n",
    "  true_value = means[action]\n",
    "\n",
    "  # draw the true value line\n",
    "  line_true_val = ax.axhline(y = true_value, color = 'b', linestyle = ':', label = \"true value\")\n",
    "\n",
    "  # plot incremental values for averaging, alpha = 0.01, alpha = 0.1\n",
    "  plt_incr_avgs, = ax.plot(incr_avgs, label = \"incremental average\")\n",
    "  plt_alpha_1_percent, = ax.plot(alpha_1_percent, label = r\"$\\alpha = 0.01$\")\n",
    "  plt_alpha_10_percent, = ax.plot(alpha_10_percent, label = r\"$\\alpha = 0.1$\")\n",
    "  plt_alpha_decay, = ax.plot(alpha_decay, label = r\"$\\alpha = decay$\")\n",
    "\n",
    "  ax.set_xlabel(\"sample number\")\n",
    "  ax.set_ylabel(\"reward value\")\n",
    "  ax.set_title(\"Incremental estimates and true expected reward values over 50 samples for action %s\" %(action + 1), y=-0.2)\n",
    "\n",
    "  # show the legend with the labels of the line\n",
    "  ax.legend(handles=[line_true_val, plt_incr_avgs, plt_alpha_1_percent, plt_alpha_10_percent, plt_alpha_decay])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMdCH4-lLw44"
   },
   "source": [
    "## Q3 Effect of $α$ on Estimated Q values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "id": "d_slkUfLUSFF"
   },
   "source": [
    "num_samples = 100\n",
    "\n",
    "# incr_avgs_by_arm has structure [[100 runs of 100 samples incr avg arm 1], [100 runs of 100 samples incr avg arm 2], [100 runs of 100 samples incr avg arm 3]]\n",
    "incr_avgs_by_arm = []\n",
    "# following three arrays have a similar structure.\n",
    "alpha_1_percent_by_arm = []\n",
    "alpha_10_percent_by_arm = []\n",
    "alpha_decay_by_arm = []\n",
    "for action in actions:\n",
    "\n",
    "  # incr_avgs_arm_run will contain the 100 runs of 100 samples for the arm we are at\n",
    "  incr_avgs_arm_run = []\n",
    "  alpha_1_percent_arm_run = []\n",
    "  alpha_10_percent_arm_run = []\n",
    "  alpha_decay_arm_run = []\n",
    "  for run in range(100):\n",
    "    rewards = [0] # the array containing the numeric samples\n",
    "    for _ in range(num_samples):\n",
    "      rewards.append(three_arm_gaussian_bandit.sample(action))\n",
    "    incr_avgs_arm_run.append(list(updateAvg(rewards)))\n",
    "    alpha_1_percent_arm_run.append(list(update(rewards, 0.01)))\n",
    "    alpha_10_percent_arm_run.append(list(update(rewards, 0.1)))\n",
    "    alpha_decay_arm_run.append(list(updateDecaying(rewards, alpha_0=0.05, lambda_=0.01, p=0.5)))\n",
    "  incr_avgs_by_arm.append(incr_avgs_arm_run)\n",
    "  alpha_1_percent_by_arm.append(alpha_1_percent_arm_run)\n",
    "  alpha_10_percent_by_arm.append(alpha_10_percent_arm_run)\n",
    "  alpha_decay_by_arm.append(alpha_decay_arm_run)\n",
    "\n",
    "# convert to np arrays:\n",
    "incr_avgs_by_arm = np.asarray(incr_avgs_by_arm)\n",
    "alpha_1_percent_by_arm = np.asarray(alpha_1_percent_by_arm)\n",
    "alpha_10_percent_by_arm = np.asarray(alpha_10_percent_by_arm)\n",
    "alpha_decay_by_arm = np.asarray(alpha_decay_by_arm)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "id": "RVFjgNjCYzt7"
   },
   "source": [
    "# our end goal is to have the structure\n",
    "# [[avg sample 1 arm 1, avg sample 2 arm 1, ..., avg sample 100 arm 1], [avg sample 1 arm 2, ..., avg sample 100 arm 2], [avg sample 1 arm 3, ..., avg sample 100 arm 3]]\n",
    "# We restructure the arrays to obtain this structure:\n",
    "avg_incr_avg_by_arm = [] #this array will contain the structure above for incr avg.\n",
    "avg_alpha_1_percent_by_arm = []\n",
    "avg_alpha_10_percent_by_arm = []\n",
    "avg_alpha_decay_by_arm = []\n",
    "\n",
    "# these arrays will have that structure for the standard deviation\n",
    "stdev_incr_avg_by_arm = []\n",
    "stdev_alpha_1_percent_by_arm = []\n",
    "stdev_alpha_10_percent_by_arm = []\n",
    "stdev_alpha_decay_by_arm = []\n",
    "\n",
    "# these arrays will have that structure for the standard error\n",
    "stderr_incr_avg_by_arm = []\n",
    "stderr_alpha_1_percent_by_arm = []\n",
    "stderr_alpha_10_percent_by_arm = []\n",
    "stderr_alpha_decay_by_arm = []\n",
    "\n",
    "for action in actions:\n",
    "  avg_incr_avg_for_arm = [] # want this to have size 101. The ith entry will be the average ith entry over all the 100 runs\n",
    "  avg_alpha_1_percent_for_arm = []\n",
    "  avg_alpha_10_percent_for_arm = []\n",
    "  avg_alpha_decay_for_arm = []\n",
    "\n",
    "  stdev_incr_avg_for_arm = [] # ith entry = standard dev of 100 ith samples\n",
    "  stdev_alpha_1_percent_for_arm = []\n",
    "  stdev_alpha_10_percent_for_arm = []\n",
    "  stdev_alpha_decay_for_arm = []\n",
    "\n",
    "  stderr_incr_avg_for_arm = [] # ith entry = standard error of 100 ith samples\n",
    "  stderr_alpha_1_percent_for_arm = []\n",
    "  stderr_alpha_10_percent_for_arm = []\n",
    "  stderr_alpha_decay_for_arm = []\n",
    "  for _ in range(101):\n",
    "    incr_avg_by_sample = [] # these will have all the samples of the same index (size 100), we'll average this\n",
    "    alpha_1_percent_by_sample = []\n",
    "    alpha_10_percent_by_sample = []\n",
    "    alpha_decay_by_sample = []\n",
    "    for run in range(100): # keep ith reward fixed, iterate over runs\n",
    "      incr_avg_by_sample.append(incr_avgs_by_arm[action][run][_])\n",
    "      alpha_1_percent_by_sample.append(alpha_1_percent_by_arm[action][run][_])\n",
    "      alpha_10_percent_by_sample.append(alpha_10_percent_by_arm[action][run][_])\n",
    "      alpha_decay_by_sample.append(alpha_decay_by_arm[action][run][_])\n",
    "\n",
    "    #convert to numpy array\n",
    "    incr_avg_by_sample = np.asarray(incr_avg_by_sample)\n",
    "    alpha_1_percent_by_sample = np.asarray(alpha_1_percent_by_sample)\n",
    "    alpha_10_percent_by_sample = np.asarray(alpha_10_percent_by_sample)\n",
    "    alpha_decay_by_sample = np.asarray(alpha_decay_by_sample)\n",
    "\n",
    "    # take average and add to vector of averages per sample index for this arm\n",
    "    avg_incr_avg_for_arm.append(np.mean(incr_avg_by_sample, axis=0))\n",
    "    avg_alpha_1_percent_for_arm.append(np.mean(alpha_1_percent_by_sample, axis=0))\n",
    "    avg_alpha_10_percent_for_arm.append(np.mean(alpha_10_percent_by_sample, axis=0))\n",
    "    avg_alpha_decay_for_arm.append(np.mean(alpha_decay_by_sample, axis=0))\n",
    "\n",
    "    # take standard deviation and add to vector of averages per sample index for this arm\n",
    "    stdev_incr_avg_for_arm.append(np.std(incr_avg_by_sample, axis=0))\n",
    "    stdev_alpha_1_percent_for_arm.append(np.std(alpha_1_percent_by_sample, axis=0))\n",
    "    stdev_alpha_10_percent_for_arm.append(np.std(alpha_10_percent_by_sample, axis=0))\n",
    "    stdev_alpha_decay_for_arm.append(np.std(alpha_decay_by_sample, axis=0))\n",
    "\n",
    "    # take standard error and add to vector of averages per sample index for this arm\n",
    "    stderr_incr_avg_for_arm.append(np.std(incr_avg_by_sample, axis=0) / np.sqrt(100))\n",
    "    stderr_alpha_1_percent_for_arm.append(np.std(alpha_1_percent_by_sample, axis=0) / np.sqrt(100))\n",
    "    stderr_alpha_10_percent_for_arm.append(np.std(alpha_10_percent_by_sample, axis=0) / np.sqrt(100))\n",
    "    stderr_alpha_decay_for_arm.append(np.std(alpha_decay_by_sample, axis=0) / np.sqrt(100))\n",
    "\n",
    "  # Add metrics for the arm to the list for all three arms.\n",
    "  avg_incr_avg_by_arm.append(avg_incr_avg_for_arm)\n",
    "  avg_alpha_1_percent_by_arm.append(avg_alpha_1_percent_for_arm)\n",
    "  avg_alpha_10_percent_by_arm.append(avg_alpha_10_percent_for_arm)\n",
    "  avg_alpha_decay_by_arm.append(avg_alpha_decay_for_arm)\n",
    "\n",
    "  stdev_incr_avg_by_arm.append(stdev_incr_avg_for_arm)\n",
    "  stdev_alpha_1_percent_by_arm.append(stdev_alpha_1_percent_for_arm)\n",
    "  stdev_alpha_10_percent_by_arm.append(stdev_alpha_10_percent_for_arm)\n",
    "  stdev_alpha_decay_by_arm.append(stdev_alpha_decay_for_arm)\n",
    "\n",
    "  stderr_incr_avg_by_arm.append(stderr_incr_avg_for_arm)\n",
    "  stderr_alpha_1_percent_by_arm.append(stderr_alpha_1_percent_for_arm)\n",
    "  stderr_alpha_10_percent_by_arm.append(stderr_alpha_10_percent_for_arm)\n",
    "  stderr_alpha_decay_by_arm.append(stderr_alpha_decay_for_arm)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaDSMygu2IZc"
   },
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EUB4pwXaRM_E",
    "outputId": "6099fc5f-efe5-4c8e-aa84-127c24201c4c"
   },
   "source": [
    "for action in actions:\n",
    "  fig, ax = plt.subplots()\n",
    "\n",
    "  # obtain averaged incremental reward values for averaging, alpha = 0.01, alpha = 0.1 and decay alpha over 100 runs\n",
    "  # TODO:\n",
    "  mean_incr_avgs_by_actions = np.asarray(avg_incr_avg_by_arm[action])\n",
    "  mean_alpha_1_percent_by_actions = np.asarray(avg_alpha_1_percent_by_arm[action])\n",
    "  mean_alpha_10_percent_by_actions = np.asarray(avg_alpha_10_percent_by_arm[action])\n",
    "  mean_alpha_decay_by_actions = np.asarray(avg_alpha_decay_by_arm[action])\n",
    "\n",
    "  true_value = means[action]\n",
    "\n",
    "  # obtain the standard deviation for averaging, alpha = 0.01, alpha = 0.1 and decay alpha over 100 runs\n",
    "  std_incr_avgs_by_actions = np.asarray(stdev_incr_avg_by_arm[action])\n",
    "  std_alpha_1_percent_by_actions = np.asarray(stdev_alpha_1_percent_by_arm[action])\n",
    "  std_alpha_10_percent_by_actions = np.asarray(stdev_alpha_10_percent_by_arm[action])\n",
    "  std_alpha_decay_by_actions = np.asarray(stdev_alpha_decay_by_arm[action])\n",
    "\n",
    "  # obtain the standard error for averaging, alpha = 0.01, alpha = 0.1 and decay alpha over 100 runs\n",
    "  std_err_incr_avgs_by_actions = np.asarray(stderr_incr_avg_by_arm[action])\n",
    "  std_err_alpha_1_percent_by_actions = np.asarray(stderr_alpha_1_percent_by_arm[action])\n",
    "  std_err_alpha_10_percent_by_actions = np.asarray(stderr_alpha_10_percent_by_arm[action])\n",
    "  std_err_alpha_decay_by_actions = np.asarray(stderr_alpha_decay_by_arm[action])\n",
    "\n",
    "  # draw the true value line\n",
    "  line_true_val = ax.axhline(y = true_value, color = 'b', linestyle = ':', label = \"true value\")\n",
    "\n",
    "  # draw the averaged incremental reward values for averaging\n",
    "  plt_incr_avgs, = ax.plot(mean_incr_avgs_by_actions, label = \"incremental average\")\n",
    "  # draw the error bar/area for averaging\n",
    "  incr_avgs_minus_std_err = mean_incr_avgs_by_actions - std_err_incr_avgs_by_actions\n",
    "  incr_avgs_plus_std_err = mean_incr_avgs_by_actions + std_err_incr_avgs_by_actions\n",
    "  ax.fill_between(range(0,101), incr_avgs_minus_std_err, incr_avgs_plus_std_err, alpha=0.3)\n",
    "\n",
    "  # draw the averaged incremental reward values for alpha = 0.01\n",
    "  plt_alpha_1_percent, = ax.plot(mean_alpha_1_percent_by_actions, label = \"alpha = 0.01\")\n",
    "  # draw the error bar/area for alpha = 0.01\n",
    "  alpha_1_percent_minus_std_err = mean_alpha_1_percent_by_actions - std_err_alpha_1_percent_by_actions\n",
    "  alpha_1_percent_plus_std_err = mean_alpha_1_percent_by_actions + std_err_alpha_1_percent_by_actions\n",
    "  ax.fill_between(range(0,101), alpha_1_percent_minus_std_err, alpha_1_percent_plus_std_err, alpha=0.3)\n",
    "\n",
    "  # draw the averaged incremental reward values for alpha = 0.1\n",
    "  plt_alpha_10_percent, = ax.plot(mean_alpha_10_percent_by_actions, label = \"alpha = 0.1\")\n",
    "  # draw the error bar/area for alpha = 0.1\n",
    "  alpha_10_percent_minus_std_err = mean_alpha_10_percent_by_actions - std_err_alpha_10_percent_by_actions\n",
    "  alpha_10_percent_plus_std_err = mean_alpha_10_percent_by_actions + std_err_alpha_10_percent_by_actions\n",
    "  ax.fill_between(range(0,101), alpha_10_percent_minus_std_err, alpha_10_percent_plus_std_err, alpha=0.3)\n",
    "\n",
    "  plt_alpha_decay, = ax.plot(mean_alpha_decay_by_actions, label = \"alpha = decay\")\n",
    "  alpha_decay_minus_std_err = mean_alpha_decay_by_actions - std_err_alpha_decay_by_actions\n",
    "  alpha_decay_plus_std_err = mean_alpha_decay_by_actions + std_err_alpha_decay_by_actions\n",
    "  ax.fill_between(range(0,101), alpha_decay_minus_std_err, alpha_decay_plus_std_err, alpha=0.3)\n",
    "\n",
    "  ax.set_xlabel(\"sample number\")\n",
    "  ax.set_ylabel(\"reward value\")\n",
    "  ax.set_title(\"Incremental estimates and true expected reward values averaged over 100 runs for action %s\" %action, y=-0.2)\n",
    "\n",
    "  ax.legend(handles=[line_true_val, plt_incr_avgs, plt_alpha_1_percent, plt_alpha_10_percent, plt_alpha_decay])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HKeI_j9cpvs"
   },
   "source": [
    "### Answers:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kg_wFpYKX53e"
   },
   "source": [
    "α = 0.1 seems to be performing better than $\\alpha$ = 0.01, which in term performs better than the decaying $\\alpha$. The way decaying $\\alpha$ is set up, it decays very slowly from 0.5 to about 0.35 after 100 iterations. Incremental averaging appears to be performing better than the fixed and decaying $\\alpha$ methods. It may be preferential to use decaying α over a longer time horizon. If we were to run further experiments, we would likely examine sequences $\\alpha_t$ which converge to 0 faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VL_sU0R5L2se"
   },
   "source": [
    "## Q4 Epsilon-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "id": "hEhRJLpKdhK0"
   },
   "source": [
    "def epsilon_greedy(bandit, epsilon, alpha = None, num_time_step = 1000, epsilon_decay=False, lambda_=0.001, stationary=True, nonstation_change=[]):\n",
    "  \"\"\"Epsilon greedy algorithm for bandit action selection\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  bandit : bandit class\n",
    "      A bernoulli bandit attributes num_arms and probs_arr, and method sample\n",
    "  epsilon: float\n",
    "      A parameter which determines the probability for a random action to be selected\n",
    "  alpha: (optional) float\n",
    "      A parameter which determined the learning rate for averaging. If alpha is none, incremental averaging is used.\n",
    "      Default is none, corresponding to incremental averaging.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  R_over_t\n",
    "      a list of instantaneous return over the time steps\n",
    "  total_R_over_t\n",
    "      a list of cummulative reward over the time steps\n",
    "  est_is_best_over_t\n",
    "      a list of values of 0 and 1 where 1 indicates the estimated best action is the true best action and 0 otherwise for each time step\n",
    "  l_over_t\n",
    "      a list of instanteneous regret over the time steps\n",
    "  total_l_over_t\n",
    "      a list of cummulative regret over the time steps\n",
    "  \"\"\"\n",
    "  # TODO:\n",
    "  num_arms = bandit.num_arms\n",
    "\n",
    "  # I think Q_arr is the array of rewards per arm and has form [[rewards for arm 1], ..., [rewards for last arm]]\n",
    "  Q_arr = np.zeros(num_arms)\n",
    "  N_arr = np.zeros(num_arms, dtype=np.int64)\n",
    "  total_R = 0\n",
    "  total_l = 0\n",
    "  actions = range(num_arms)\n",
    "\n",
    "  opt_value =  np.max(bandit.mean)\n",
    "  best_action = np.argmax(bandit.mean)\n",
    "\n",
    "  # I will add this extra array which will keep track of the rewards per arm\n",
    "  # R_per_arm = [[] for _ in range(num_arms)]\n",
    "  R_per_arm = np.zeros((num_arms, num_time_step))\n",
    "\n",
    "  R_over_t = []\n",
    "  total_R_over_t = []\n",
    "  est_is_best_over_t = []\n",
    "  l_over_t = []\n",
    "  total_l_over_t = []\n",
    "\n",
    "  epsilon_t = epsilon\n",
    "\n",
    "  updates = []\n",
    "  for arm in range(num_arms):\n",
    "    if (alpha == None):\n",
    "      updates.append(updateAvg(R_per_arm[arm]))\n",
    "    else:\n",
    "      updates.append(update(R_per_arm[arm], alpha))\n",
    "\n",
    "  for time_step in range(num_time_step):\n",
    "    if not stationary:\n",
    "      if time_step == nonstation_change[0]:\n",
    "        bandit = nonstation_change[1]\n",
    "\n",
    "    if epsilon_decay:\n",
    "        epsilon_t = epsilon/(1+lambda_*time_step)\n",
    "\n",
    "    # random_choice = np.random.choice([True, False], p=[epsilon_t, 1-epsilon_t])\n",
    "    if np.random.uniform() <= epsilon_t:\n",
    "      A = np.random.choice(actions)\n",
    "    else:\n",
    "      indices_of_best_actions = (Q_arr == np.max(Q_arr)).nonzero() # if multiple actions have optimal average, choose one at random\n",
    "      A = np.random.choice(indices_of_best_actions[0])\n",
    "\n",
    "    # this will be for the non-stationary bandit:\n",
    "    # if (not stationary) and (sum(N_arr)>500):\n",
    "    #   bandit.mean = [0.5, 0.5+2*delta, 0.5-2*delta]\n",
    "\n",
    "    curr_R = bandit.sample(A)\n",
    "\n",
    "    #added:\n",
    "    # R_per_arm[A].append(curr_R)\n",
    "    R_per_arm[A][N_arr[A]] = curr_R\n",
    "\n",
    "    N_arr[A] += 1\n",
    "    Q_arr[A] = next(updates[A])\n",
    "\n",
    "\n",
    "    total_R += curr_R\n",
    "    total_l += opt_value - curr_R\n",
    "\n",
    "    R_over_t.append(curr_R)\n",
    "\n",
    "    total_R += curr_R\n",
    "    total_R_over_t.append(total_R)\n",
    "\n",
    "    est_is_best = int(A == np.argmax(bandit.mean))\n",
    "    est_is_best_over_t.append(est_is_best)\n",
    "\n",
    "    l_t = opt_value - curr_R\n",
    "    l_over_t.append(l_t)\n",
    "\n",
    "    total_l += l_t\n",
    "    total_l_over_t.append(total_l)\n",
    "  return R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XU1_pP7INeBH"
   },
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JgvBlfQDBJdv",
    "outputId": "30b7b7ba-a969-4207-9ff9-1aed830d5ad4"
   },
   "source": [
    "#TODO:\n",
    "epsilons = [0, 1/8, 1/4, 1/2, 1]\n",
    "decaying_epsilon_params = {'epsilon_0': 0.5, 'lambda_': 0.1}  # Decaying epsilon parameters\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(18, 18))\n",
    "\n",
    "for epsilon in epsilons + [\"decay\"]:\n",
    "\n",
    "  # arrays of the data generated from 100 runs\n",
    "  R_over_t_runs = []\n",
    "  total_R_over_t_runs = []\n",
    "  est_is_best_over_t_runs = []\n",
    "  l_over_t_runs = []\n",
    "  total_l_over_t_runs = []\n",
    "\n",
    "  for run in range(100):\n",
    "    if epsilon == \"decay\":\n",
    "      R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = epsilon_greedy(\n",
    "          three_arm_gaussian_bandit,\n",
    "          decaying_epsilon_params['epsilon_0'],\n",
    "          epsilon_decay=True,\n",
    "          lambda_=decaying_epsilon_params['lambda_']\n",
    "      )\n",
    "    else:\n",
    "      R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = epsilon_greedy(\n",
    "          three_arm_gaussian_bandit,\n",
    "          epsilon\n",
    "      )\n",
    "    R_over_t_runs.append(R_over_t)\n",
    "    total_R_over_t_runs.append(total_R_over_t)\n",
    "    est_is_best_over_t_runs.append(est_is_best_over_t)\n",
    "    l_over_t_runs.append(l_over_t)\n",
    "    total_l_over_t_runs.append(total_l_over_t)\n",
    "\n",
    "  R_over_t_runs = np.asarray(R_over_t_runs)\n",
    "  total_R_over_t_runs = np.asarray(total_R_over_t_runs)\n",
    "  est_is_best_over_t_runs = np.asarray(est_is_best_over_t_runs)\n",
    "  l_over_t_runs = np.asarray(l_over_t_runs)\n",
    "  total_l_over_t_runs = np.asarray(total_l_over_t_runs)\n",
    "\n",
    "  # plot the mean reward over time\n",
    "\n",
    "  mean_R_over_t_runs = np.mean(R_over_t_runs, axis=0)\n",
    "  # print(mean_R_over_t_runs)\n",
    "  std_err_R_over_t_runs = np.std(R_over_t_runs, axis=0) / np.sqrt(np.size(R_over_t_runs, axis=0))\n",
    "\n",
    "  axs[0,0].plot(mean_R_over_t_runs, label = r\"$\\epsilon = %s$\" %epsilon)\n",
    "\n",
    "  R_over_t_minus_std_err = mean_R_over_t_runs - std_err_R_over_t_runs\n",
    "  R_over_t_plus_std_err = mean_R_over_t_runs  + std_err_R_over_t_runs\n",
    "  axs[0,0].fill_between(range(0,1000), R_over_t_minus_std_err, R_over_t_plus_std_err, alpha=0.4)\n",
    "  # axs[0,0].errorbar(range(0,1000), mean_R_over_t_runs, yerr=std_err_R_over_t_runs)\n",
    "\n",
    "  axs[0,0].legend()\n",
    "  axs[0,0].set_xlabel(\"time step\")\n",
    "  axs[0,0].set_ylabel(\"reward value\")\n",
    "  axs[0,0].set_title(\"Average Instantaneous Reward Received over Time\", y=-0.18)\n",
    "\n",
    "  # plot the mean cummulative reward over time\n",
    "\n",
    "  mean_total_R_over_t_runs = np.mean(total_R_over_t_runs, axis=0) # different epsilons seem to be performing exactly the same\n",
    "  std_err_total_R_over_t_runs = np.std(total_R_over_t_runs, axis=0) / np.sqrt(np.size(total_R_over_t_runs, axis=0))\n",
    "\n",
    "  axs[0,1].plot(mean_total_R_over_t_runs, label = r\"$\\epsilon = %s$\" %epsilon)\n",
    "\n",
    "  total_R_over_t_minus_std_err = mean_total_R_over_t_runs - std_err_total_R_over_t_runs\n",
    "  total_R_over_t_plus_std_err = mean_total_R_over_t_runs  + std_err_total_R_over_t_runs\n",
    "  axs[0,1].fill_between(range(0,1000), total_R_over_t_minus_std_err, total_R_over_t_plus_std_err, alpha=0.4)\n",
    "\n",
    "  axs[0,1].legend()\n",
    "  axs[0,1].set_xlabel(\"time step\")\n",
    "  axs[0,1].set_ylabel(\"reward value\")\n",
    "  axs[0,1].set_title(\"Average Cumulative Reward Received over Time\", y=-0.18)\n",
    "\n",
    "  #plot the mean percentage of the estimated best action being the third action\n",
    "\n",
    "  est_is_best_over_t_runs_avgs = np.mean(est_is_best_over_t_runs, axis=0)\n",
    "  plt_est_is_best_over_t_runs_avgs, = axs[1,0].plot(est_is_best_over_t_runs_avgs, label = r\"$\\epsilon = %s$\" %epsilon)\n",
    "\n",
    "  axs[1,0].legend()\n",
    "  axs[1,0].set_xlabel(\"time step\")\n",
    "  axs[1,0].set_ylabel(\"percentage\")\n",
    "  axs[1,0].set_title(\"Percentage of Runs where Best Action was Chosen\", y=-0.18)\n",
    "\n",
    "  #plot the mean instantaneous regret over time\n",
    "\n",
    "  l_over_t_runs_avgs = np.mean(l_over_t_runs, axis=0)\n",
    "  axs[1,1].plot(l_over_t_runs_avgs, label = r\"$\\epsilon = %s$\" %epsilon)\n",
    "\n",
    "  axs[1,1].legend()\n",
    "  axs[1,1].set_xlabel(\"time step\")\n",
    "  axs[1,1].set_ylabel(\"regret\")\n",
    "  axs[1,1].set_title(\"Instantaneous Regret over Time\", y=-0.18)\n",
    "\n",
    "  #plot the total regret over time\n",
    "\n",
    "  total_l_over_t_runs_avgs = np.mean(total_l_over_t_runs, axis=0)\n",
    "  axs[2,0].plot(total_l_over_t_runs_avgs, label = r\"$\\epsilon = %s$\" %epsilon)\n",
    "\n",
    "  axs[2,0].legend()\n",
    "  axs[2,0].set_xlabel(\"time step\")\n",
    "  axs[2,0].set_ylabel(\"regret\")\n",
    "  axs[2,0].set_title(\"Total Regret up to Time Step t\", y=-0.18)\n",
    "\n",
    "axs[-1, -1].axis('off')\n",
    "\n",
    "title = r'Graphs  for Epsilon Greedy with Varying Epsilons'\n",
    "fig.suptitle(title, fontsize=16, y=0.08)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILoBgOrocu_z"
   },
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnrJI7uKAvkH"
   },
   "source": [
    "Because the first observed reward is highly likely to be positive, in the strictly greedy ($\\epsilon=0$) case, the agent ends up pulling the same arm for the rest of the run time, which explains the poor performance in that case. $\\epsilon = 1$ similarly performs poorly because the estimates of the means values of the arms are never used to make a decision about which arm to sample. Smaller $\\epsilon$ values seem to perform better over time, with decaying $\\epsilon$ (which roughly decays from $0.5$ to $0.005$) accumulating less regret over time than $\\epsilon = \\frac{1}{8}$, which in turn accumulates less regret than $\\epsilon = \\frac{1}{4}$ and $\\epsilon = \\frac{1}{2}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qULufDuyXgb"
   },
   "source": [
    "## Q5 Hyperparameters for Epsilon-greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVbJAAc0Ebc6"
   },
   "source": [
    "To have a plain start, you have been provided with predefined functions for generating plots until now. However, moving forward, you are expected to plot graphs on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "isIB2IwAwzX4"
   },
   "source": [
    "# how we're gonna go about this:\n",
    "# By column. Each column will correspond to a different alpha value.\n",
    "# Each graph will have the same 3 base curves: greedy non-decaying with epsilon [1/4,1/8] and greedy decaying with epsilon_0=1/2 and lambda=0.1\n",
    "# Then we can add to each the specific curve of decaying with fixed learning rate, this will vary by column"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jH8EgaKmvEbz"
   },
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GmwDt2_JBAwH",
    "outputId": "ed8d1b11-3cd1-49db-8146-8b99c9567e80"
   },
   "source": [
    "alphas = [0.1, 0.01, 0.001, None] # none represents incremental averaging\n",
    "\n",
    "\n",
    "# we start by making the base graphs! This procedure is almost identical to #4, but with the row and column structure a bit different\n",
    "epsilons = [1/4, 1/8, 'decay']\n",
    "decaying_epsilon_params = {'epsilon_0': 0.5, 'lambda_': 0.1}  # Decaying epsilon parameters\n",
    "\n",
    "num_rows = 5\n",
    "num_cols = 3\n",
    "\n",
    "fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols,constrained_layout=True, figsize=(30, 30))\n",
    "\n",
    "\n",
    "for epsilon in range(len(epsilons)):\n",
    "  row_1 = []\n",
    "  row_2 = []\n",
    "  row_3 = []\n",
    "  row_4 = []\n",
    "  row_5 = []\n",
    "  for alp in range(len(alphas)):\n",
    "    # arrays of the data generated from 100 runs\n",
    "    R_over_t_runs = []\n",
    "    total_R_over_t_runs = []\n",
    "    est_is_best_over_t_runs = []\n",
    "    l_over_t_runs = []\n",
    "    total_l_over_t_runs = []\n",
    "\n",
    "    for run in range(100):\n",
    "      if epsilons[epsilon] == 'decay':\n",
    "        R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = epsilon_greedy(\n",
    "            three_arm_gaussian_bandit,\n",
    "            alpha=alphas[alp],\n",
    "            epsilon=decaying_epsilon_params['epsilon_0'],\n",
    "            epsilon_decay=True,\n",
    "            lambda_=decaying_epsilon_params['lambda_']\n",
    "        )\n",
    "      else:\n",
    "        R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = epsilon_greedy(\n",
    "            three_arm_gaussian_bandit,\n",
    "            epsilons[epsilon],\n",
    "            alpha=alphas[alp]\n",
    "        )\n",
    "      R_over_t_runs.append(R_over_t)\n",
    "      total_R_over_t_runs.append(total_R_over_t)\n",
    "      est_is_best_over_t_runs.append(est_is_best_over_t)\n",
    "      l_over_t_runs.append(l_over_t)\n",
    "      total_l_over_t_runs.append(total_l_over_t)\n",
    "\n",
    "    R_over_t_runs = np.asarray(R_over_t_runs)\n",
    "    total_R_over_t_runs = np.asarray(total_R_over_t_runs)\n",
    "    est_is_best_over_t_runs = np.asarray(est_is_best_over_t_runs)\n",
    "    l_over_t_runs = np.asarray(l_over_t_runs)\n",
    "    total_l_over_t_runs = np.asarray(total_l_over_t_runs)\n",
    "\n",
    "    # plot the mean reward over time\n",
    "\n",
    "    mean_R_over_t_runs = np.mean(R_over_t_runs, axis=0)\n",
    "    std_err_R_over_t_runs = np.std(R_over_t_runs, axis=0) / np.sqrt(np.size(R_over_t_runs, axis=0))\n",
    "\n",
    "    axs[0,epsilon].plot(mean_R_over_t_runs, label = r\"$\\alpha = %s$\" %alphas[alp])\n",
    "\n",
    "    R_over_t_minus_std_err = mean_R_over_t_runs - std_err_R_over_t_runs\n",
    "    R_over_t_plus_std_err = mean_R_over_t_runs  + std_err_R_over_t_runs\n",
    "    axs[0,epsilon].fill_between(range(0,1000), R_over_t_minus_std_err, R_over_t_plus_std_err, alpha=0.4)\n",
    "    # axs[0,0].errorbar(range(0,1000), mean_R_over_t_runs, yerr=std_err_R_over_t_runs)\n",
    "\n",
    "    axs[0,epsilon].legend()\n",
    "    axs[0,epsilon].set_xlabel(\"time step\")\n",
    "    axs[0,epsilon].set_ylabel(\"reward value\")\n",
    "    axs[0,epsilon].set_title(\"Average Instantaneous Reward Received over Time with ε= \"+str(epsilons[epsilon]), y=-0.18)\n",
    "\n",
    "    # plot the mean cummulative reward over time\n",
    "\n",
    "    mean_total_R_over_t_runs = np.mean(total_R_over_t_runs, axis=0)\n",
    "    std_err_total_R_over_t_runs = np.std(total_R_over_t_runs, axis=0) / np.sqrt(np.size(total_R_over_t_runs, axis=0))\n",
    "\n",
    "    axs[1,epsilon].plot(mean_total_R_over_t_runs, label = r\"$\\alpha = %s$\" %alphas[alp])\n",
    "\n",
    "    total_R_over_t_minus_std_err = mean_total_R_over_t_runs - std_err_total_R_over_t_runs\n",
    "    total_R_over_t_plus_std_err = mean_total_R_over_t_runs  + std_err_total_R_over_t_runs\n",
    "    axs[1,epsilon].fill_between(range(0,1000), total_R_over_t_minus_std_err, total_R_over_t_plus_std_err, alpha=0.4)\n",
    "\n",
    "    axs[1,epsilon].legend()\n",
    "    axs[1,epsilon].set_xlabel(\"time step\")\n",
    "    axs[1,epsilon].set_ylabel(\"reward value\")\n",
    "    axs[1,epsilon].set_title(\"Average Cumulative Reward Received over Time with ε= \"+str(epsilons[epsilon]), y=-0.18)\n",
    "\n",
    "    #plot the mean percentage of the estimated best action being the third action\n",
    "\n",
    "    est_is_best_over_t_runs_avgs = np.mean(est_is_best_over_t_runs, axis=0)\n",
    "    # plt_est_is_best_over_t_runs_avgs, = axs[2,epsilon].plot(est_is_best_over_t_runs_avgs, label = r\"$\\alpha = %s$\" %alphas[alp])\n",
    "    axs[2,epsilon].plot(est_is_best_over_t_runs_avgs, label = r\"$\\alpha = %s$\" %alphas[alp])\n",
    "\n",
    "    axs[2,epsilon].legend()\n",
    "    axs[2,epsilon].set_xlabel(\"time step\")\n",
    "    axs[2,epsilon].set_ylabel(\"percentage\")\n",
    "    axs[2,epsilon].set_title(\"Percentage of Runs where Best Action was Chosen with ε= \"+str(epsilons[epsilon]), y=-0.18)\n",
    "\n",
    "    #plot the mean instantaneous regret over time\n",
    "\n",
    "    l_over_t_runs_avgs = np.mean(l_over_t_runs, axis=0)\n",
    "    axs[3,epsilon].plot(l_over_t_runs_avgs, label = r\"$\\alpha = %s$\" %alphas[alp])\n",
    "\n",
    "    axs[3,epsilon].legend()\n",
    "    axs[3,epsilon].set_xlabel(\"time step\")\n",
    "    axs[3,epsilon].set_ylabel(\"regret\")\n",
    "    axs[3,epsilon].set_title(\"Instantaneous Regret over Time with ε=\"+str(epsilons[epsilon]), y=-0.18)\n",
    "\n",
    "    #plot the total regret over time\n",
    "\n",
    "    total_l_over_t_runs_avgs = np.mean(total_l_over_t_runs, axis=0)\n",
    "    axs[4,epsilon].plot(total_l_over_t_runs_avgs, label = r\"$\\alpha = %s$\" %str(alphas[alp]))\n",
    "\n",
    "    axs[4,epsilon].legend()\n",
    "    axs[4,epsilon].set_xlabel(\"time step\")\n",
    "    axs[4,epsilon].set_ylabel(\"regret\")\n",
    "    axs[4,epsilon].set_title(r\"Total Regret up to Time Step t with ε= \"+str(epsilons[epsilon]), y=-0.18)\n",
    "\n",
    "  axs[-1, -1].axis('off')\n",
    "\n",
    "title = r'Graphs  for Epsilon Greedy with Varying Epsilons'\n",
    "fig.suptitle(title, fontsize=16, y=-0.08)\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRVYgAJPczbo"
   },
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzcTHHnbEZbZ"
   },
   "source": [
    "Incremental averaging and $\\alpha = 0.1$ exhibit slow growth total regret (though still roughly linear), whereas smaller learning rates ($\\alpha = 0.01, 0.001$) give linear growth in total regret. Incremental averaging seems to learn early on which action is best, and then $\\alpha = 0.1$ tends to catch up after a bit of a lag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5ttd7oJXiQe"
   },
   "source": [
    "## Q6 Gradient Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "id": "Wed5NdLZXjrE"
   },
   "source": [
    "def gradient_bandit(bandit, alpha, num_time_steps=1000, alpha_decay=False, lda=0.01, p=0.5, stationary=True, nonstation_change=[]):\n",
    "    instantaneous_rewards_ot = []\n",
    "    cumulative_rewards_ot = np.zeros(num_time_steps)\n",
    "    estimate_of_best_action_ot = np.zeros(num_time_steps)\n",
    "    instantaneous_regret_ot = np.zeros(num_time_steps)\n",
    "    cumulative_regret_ot = np.zeros(num_time_steps)\n",
    "    N_arr = np.zeros(bandit.num_arms) # added to keep track of number of tries (for non-stationary)\n",
    "\n",
    "    total_regret = 0\n",
    "\n",
    "    H = np.zeros(bandit.num_arms)\n",
    "    pi = softmax(H) # parameter p for np.random.choice\n",
    "\n",
    "    best_action = np.argmax(bandit.mean)\n",
    "    optimal_value = np.max(bandit.mean)\n",
    "\n",
    "    for i in range(num_time_steps):\n",
    "        alpha_t = alpha\n",
    "        if alpha_decay:\n",
    "          alpha_t /= (1 + lda*i) ** p\n",
    "\n",
    "        A_t = np.random.choice(range(bandit.num_arms), p=pi) # selects action according to probabilities in pi\n",
    "\n",
    "        # two lines for the non-stationary bandit:\n",
    "        N_arr[A_t] += 1\n",
    "        # if (not stationary) and (sum(N_arr)>500):\n",
    "        #   bandit.mean = [0.5, 0.5+2*delta, 0.5-2*delta]\n",
    "        if not stationary:\n",
    "          if i == nonstation_change[0]:\n",
    "            bandit = nonstation_change[1]\n",
    "\n",
    "        R_t = bandit.sample(A_t)\n",
    "        instantaneous_rewards_ot.append(R_t)\n",
    "        if (i > 0):\n",
    "          cumulative_rewards_ot[i] = cumulative_rewards_ot[i-1] + R_t\n",
    "        else:\n",
    "          cumulative_rewards_ot[i] = R_t\n",
    "        average_R_t = next(updateAvg(instantaneous_rewards_ot))\n",
    "        estimate_of_best_action_ot[i] = int(best_action == A_t)\n",
    "\n",
    "        H[A_t] = H[A_t] + alpha_t*(R_t - average_R_t)*(1 - pi[A_t])\n",
    "        for a in range(bandit.num_arms):\n",
    "            if a != A_t:\n",
    "                H[a] = H[a] - alpha_t*(R_t - average_R_t)*pi[a]\n",
    "        pi = softmax(H)\n",
    "\n",
    "        instantaneous_regret = optimal_value - R_t\n",
    "        instantaneous_regret_ot[i] = instantaneous_regret\n",
    "        total_regret += instantaneous_regret\n",
    "        cumulative_regret_ot[i] = total_regret\n",
    "\n",
    "    return instantaneous_rewards_ot, cumulative_rewards_ot, estimate_of_best_action_ot, instantaneous_regret_ot, cumulative_regret_ot\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqUx-AYVvu2B"
   },
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_wiqI1D2FlWY",
    "outputId": "9ff680bf-1788-49a1-e9d2-63299fdb0bf9"
   },
   "source": [
    "alphas = [0.1, 0.01, 0.001]\n",
    "decaying_alpha_params = {'alpha_0': 0.5, 'lda': 0.01, 'p' : 0.5}\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(18, 18))\n",
    "\n",
    "for alp in alphas + [\"decay\"]:\n",
    "\n",
    "  # arrays of the data generated from 100 runs\n",
    "  R_over_t_runs = []\n",
    "  total_R_over_t_runs = []\n",
    "  est_is_best_over_t_runs = []\n",
    "  l_over_t_runs = []\n",
    "  total_l_over_t_runs = []\n",
    "\n",
    "  for run in range(100):\n",
    "    if alp == \"decay\":\n",
    "      R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = gradient_bandit(\n",
    "          three_arm_gaussian_bandit,\n",
    "          alpha=decaying_alpha_params['alpha_0'],\n",
    "          alpha_decay=True,\n",
    "          lda=decaying_alpha_params['lda'],\n",
    "          p=decaying_alpha_params['p']\n",
    "          )\n",
    "    else:\n",
    "      R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = gradient_bandit(\n",
    "          three_arm_gaussian_bandit,\n",
    "          alp\n",
    "      )\n",
    "    R_over_t_runs.append(R_over_t)\n",
    "    total_R_over_t_runs.append(total_R_over_t)\n",
    "    est_is_best_over_t_runs.append(est_is_best_over_t)\n",
    "    l_over_t_runs.append(l_over_t)\n",
    "    total_l_over_t_runs.append(total_l_over_t)\n",
    "\n",
    "  R_over_t_runs = np.asarray(R_over_t_runs)\n",
    "  total_R_over_t_runs = np.asarray(total_R_over_t_runs)\n",
    "  est_is_best_over_t_runs = np.asarray(est_is_best_over_t_runs)\n",
    "  l_over_t_runs = np.asarray(l_over_t_runs)\n",
    "  total_l_over_t_runs = np.asarray(total_l_over_t_runs)\n",
    "\n",
    "  # plot the mean reward over time\n",
    "\n",
    "  mean_R_over_t_runs = np.mean(R_over_t_runs, axis=0)\n",
    "  # print(mean_R_over_t_runs)\n",
    "  std_err_R_over_t_runs = np.std(R_over_t_runs, axis=0) / np.sqrt(np.size(R_over_t_runs, axis=0))\n",
    "\n",
    "  axs[0,0].plot(mean_R_over_t_runs, label = r\"$\\alpha = %s$\" %alp)\n",
    "\n",
    "  R_over_t_minus_std_err = mean_R_over_t_runs - std_err_R_over_t_runs\n",
    "  R_over_t_plus_std_err = mean_R_over_t_runs  + std_err_R_over_t_runs\n",
    "  axs[0,0].fill_between(range(0,1000), R_over_t_minus_std_err, R_over_t_plus_std_err, alpha=0.4)\n",
    "  # axs[0,0].errorbar(range(0,1000), mean_R_over_t_runs, yerr=std_err_R_over_t_runs)\n",
    "\n",
    "  axs[0,0].legend()\n",
    "  axs[0,0].set_xlabel(\"time step\")\n",
    "  axs[0,0].set_ylabel(\"reward value\")\n",
    "  axs[0,0].set_title(\"Average Instanteneous Reward Received over Time\", y=-0.18)\n",
    "\n",
    "  # plot the mean cummulative reward over time\n",
    "\n",
    "  mean_total_R_over_t_runs = np.mean(total_R_over_t_runs, axis=0)\n",
    "  std_err_total_R_over_t_runs = np.std(total_R_over_t_runs, axis=0) / np.sqrt(np.size(total_R_over_t_runs, axis=0))\n",
    "\n",
    "  axs[0,1].plot(mean_total_R_over_t_runs, label = r\"$\\alpha = %s$\" %alp)\n",
    "\n",
    "  total_R_over_t_minus_std_err = mean_total_R_over_t_runs - std_err_total_R_over_t_runs\n",
    "  total_R_over_t_plus_std_err = mean_total_R_over_t_runs  + std_err_total_R_over_t_runs\n",
    "  axs[0,1].fill_between(range(0,1000), total_R_over_t_minus_std_err, total_R_over_t_plus_std_err, alpha=0.4)\n",
    "\n",
    "  axs[0,1].legend()\n",
    "  axs[0,1].set_xlabel(\"time step\")\n",
    "  axs[0,1].set_ylabel(\"reward value\")\n",
    "  axs[0,1].set_title(\"Average Cumulative Reward Received over Time\", y=-0.18)\n",
    "\n",
    "  #plot the mean percentage of the estimated best action being the third action\n",
    "\n",
    "  est_is_best_over_t_runs_avgs = np.mean(est_is_best_over_t_runs, axis=0)\n",
    "  plt_est_is_best_over_t_runs_avgs, = axs[1,0].plot(est_is_best_over_t_runs_avgs, label = r\"$\\alpha = %s$\" %alp)\n",
    "\n",
    "  axs[1,0].legend()\n",
    "  axs[1,0].set_xlabel(\"time step\")\n",
    "  axs[1,0].set_ylabel(\"percentage\")\n",
    "  axs[1,0].set_title(\"Percentage of Runs where Best Action was Chosen\", y=-0.18)\n",
    "\n",
    "  #plot the mean instantaneous regret over time\n",
    "\n",
    "  l_over_t_runs_avgs = np.mean(l_over_t_runs, axis=0)\n",
    "  axs[1,1].plot(l_over_t_runs_avgs, label = r\"$\\alpha = %s$\" %alp)\n",
    "\n",
    "  axs[1,1].legend()\n",
    "  axs[1,1].set_xlabel(\"time step\")\n",
    "  axs[1,1].set_ylabel(\"regret\")\n",
    "  axs[1,1].set_title(\"Instantaneous Regret over Time\", y=-0.18)\n",
    "\n",
    "  #plot the total regret over time\n",
    "\n",
    "  total_l_over_t_runs_avgs = np.mean(total_l_over_t_runs, axis=0)\n",
    "  axs[2,0].plot(total_l_over_t_runs_avgs, label = r\"$\\alpha = %s$\" %alp)\n",
    "\n",
    "  axs[2,0].legend()\n",
    "  axs[2,0].set_xlabel(\"time step\")\n",
    "  axs[2,0].set_ylabel(\"regret\")\n",
    "  axs[2,0].set_title(\"Total Regret up to Time Step t\", y=-0.18)\n",
    "\n",
    "axs[-1, -1].axis('off')\n",
    "\n",
    "title = r'Graphs for Gradient Bandit with Varying Learning Rates'\n",
    "fig.suptitle(title, fontsize=16, y=0.08)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NABR2XVSc2fk"
   },
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xw4DpXavIoue"
   },
   "source": [
    "Starting off with a higher learning rate seems to give better performance. When $\\alpha = 0.1$ or $\\alpha = 0.01$, the updates to the preferences occur slowly, which seems to give linear growth in total regret. However, if $\\alpha = 0.1$ or we initialize our decaying $\\alpha_t$ to 0.5, the preferences are updates enough to achieve logarithmic (i.e. optimal) accumulation of regret.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9GklWhZM9um"
   },
   "source": [
    "## Q7 Thompson Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "CAlN8q-YM_Mt"
   },
   "source": [
    "def thompson_sampling(bandit, prior_means=[], prior_sd=[], num_iterations=1000, stationary=True, nonstation_change = []):\n",
    "    if (len(prior_means) == 0):\n",
    "      # prior_means = np.zeros(bandit.num_arms)\n",
    "      prior_means = np.full(bandit.num_arms, 0.000)\n",
    "\n",
    "    if (len(prior_sd) == 0):\n",
    "      # prior_means = np.zeros(bandit.num_arms)\n",
    "      prior_sd = np.full(bandit.num_arms, 1.0000)\n",
    "    instantaneous_rewards_ot = []\n",
    "    cumulative_rewards_ot = np.zeros(num_iterations)\n",
    "    estimate_of_best_action_ot = np.zeros(num_iterations)\n",
    "    instantaneous_regret_ot = np.zeros(num_iterations)\n",
    "    cumulative_regret_ot = np.zeros(num_iterations)\n",
    "\n",
    "    optimal_action = np.argmax(bandit.mean)\n",
    "    optimal_value = np.max(bandit.mean)\n",
    "    cumulative_reward = 0\n",
    "    cumulative_regret = 0\n",
    "\n",
    "    means_estimate = np.zeros(bandit.num_arms)\n",
    "    sd_estimate = 0.1\n",
    "\n",
    "    # for j in range(bandit.num_arms):\n",
    "    #     means_estimate[j] = np.random.normal(prior_means[j], prior_sd)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "      if not stationary:\n",
    "        if nonstation_change[0] == i:\n",
    "          bandit = nonstation_change[1]\n",
    "      # print(prior_sd)\n",
    "      for j in range(bandit.num_arms):\n",
    "        means_estimate[j] = np.random.normal(prior_means[j], math.sqrt(prior_sd[j] ** 2 + sd_estimate ** 2))\n",
    "        # means_estimate[j] = np.random.normal(prior_means[j], prior_sd[j])\n",
    "\n",
    "      indices_of_best_actions = (means_estimate == np.max(means_estimate)).nonzero() # if multiple actions have optimal average, choose one at random\n",
    "      # print(indices_of_best_actions)\n",
    "      A_i = np.random.choice(indices_of_best_actions[0])\n",
    "      # print(A_i)\n",
    "      # A_i = np.argmax(means_estimate)\n",
    "      R_i = bandit.sample(A_i)\n",
    "      numerator = (prior_means[A_i]/(prior_sd[A_i] ** 2)) + (R_i/(sd_estimate ** 2))\n",
    "      denominator = 1/(prior_sd[A_i] ** 2) + 1/(sd_estimate ** 2)\n",
    "      prior_means[A_i] = numerator/denominator\n",
    "      prior_sd[A_i] = np.sqrt(1/(1/(sd_estimate ** 2) + 1/(prior_sd[A_i] ** 2)))\n",
    "\n",
    "      instantaneous_rewards_ot.append(R_i)\n",
    "      cumulative_reward += R_i\n",
    "      cumulative_rewards_ot[i] = cumulative_reward\n",
    "      estimate_of_best_action_ot[i] = int(A_i == optimal_action)\n",
    "      inst_regret = optimal_value - R_i\n",
    "      instantaneous_regret_ot[i] = inst_regret\n",
    "      cumulative_regret += inst_regret\n",
    "      cumulative_regret_ot[i] = cumulative_regret\n",
    "\n",
    "    # print(means_estimate)\n",
    "    return instantaneous_rewards_ot, cumulative_rewards_ot, estimate_of_best_action_ot, instantaneous_regret_ot, cumulative_regret_ot\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true,
    "id": "LZcqLu55ZM0N"
   },
   "source": [
    "instantaneous_rewards_ot, cumulative_rewards_ot, estimate_of_best_action_ot, instantaneous_regret_ot, cumulative_regret_ot = thompson_sampling(three_arm_gaussian_bandit)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG3KAgC5v2gl"
   },
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tNW238UNONAz",
    "outputId": "877993ae-49c8-4624-afb6-90c5d99d92da"
   },
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(18, 18))\n",
    "\n",
    "R_over_t_runs = []\n",
    "total_R_over_t_runs = []\n",
    "est_is_best_over_t_runs = []\n",
    "l_over_t_runs = []\n",
    "total_l_over_t_runs = []\n",
    "\n",
    "for run in range(100):\n",
    "    R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = thompson_sampling(three_arm_gaussian_bandit)\n",
    "    R_over_t_runs.append(R_over_t)\n",
    "    total_R_over_t_runs.append(total_R_over_t)\n",
    "    est_is_best_over_t_runs.append(est_is_best_over_t)\n",
    "    l_over_t_runs.append(l_over_t)\n",
    "    total_l_over_t_runs.append(total_l_over_t)\n",
    "\n",
    "R_over_t_runs = np.asarray(R_over_t_runs)\n",
    "total_R_over_t_runs = np.asarray(total_R_over_t_runs)\n",
    "est_is_best_over_t_runs = np.asarray(est_is_best_over_t_runs)\n",
    "l_over_t_runs = np.asarray(l_over_t_runs)\n",
    "total_l_over_t_runs = np.asarray(total_l_over_t_runs)\n",
    "\n",
    "# plot the mean reward over time\n",
    "\n",
    "mean_R_over_t_runs = np.mean(R_over_t_runs, axis=0)\n",
    "# print(mean_R_over_t_runs)\n",
    "std_err_R_over_t_runs = np.std(R_over_t_runs, axis=0) / np.sqrt(np.size(R_over_t_runs, axis=0))\n",
    "\n",
    "axs[0,0].plot(mean_R_over_t_runs)\n",
    "\n",
    "R_over_t_minus_std_err = mean_R_over_t_runs - std_err_R_over_t_runs\n",
    "R_over_t_plus_std_err = mean_R_over_t_runs  + std_err_R_over_t_runs\n",
    "axs[0,0].fill_between(range(0,1000), R_over_t_minus_std_err, R_over_t_plus_std_err, alpha=0.4)\n",
    "# axs[0,0].errorbar(range(0,1000), mean_R_over_t_runs, yerr=std_err_R_over_t_runs)\n",
    "\n",
    "# axs[0,0].legend()\n",
    "axs[0,0].set_xlabel(\"time step\")\n",
    "axs[0,0].set_ylabel(\"reward value\")\n",
    "axs[0,0].set_title(\"Average Instanteneous Reward Received over Time\", y=-0.18)\n",
    "\n",
    "# plot the mean cummulative reward over time\n",
    "\n",
    "mean_total_R_over_t_runs = np.mean(total_R_over_t_runs, axis=0)\n",
    "std_err_total_R_over_t_runs = np.std(total_R_over_t_runs, axis=0) / np.sqrt(np.size(total_R_over_t_runs, axis=0))\n",
    "\n",
    "axs[0,1].plot(mean_total_R_over_t_runs)\n",
    "\n",
    "total_R_over_t_minus_std_err = mean_total_R_over_t_runs - std_err_total_R_over_t_runs\n",
    "total_R_over_t_plus_std_err = mean_total_R_over_t_runs  + std_err_total_R_over_t_runs\n",
    "axs[0,1].fill_between(range(0,1000), total_R_over_t_minus_std_err, total_R_over_t_plus_std_err, alpha=0.4)\n",
    "\n",
    "# axs[0,1].legend()\n",
    "axs[0,1].set_xlabel(\"time step\")\n",
    "axs[0,1].set_ylabel(\"reward value\")\n",
    "axs[0,1].set_title(\"Average Cumulative Reward Received over Time\", y=-0.18)\n",
    "\n",
    "#plot the mean percentage of the estimated best action being the third action\n",
    "\n",
    "est_is_best_over_t_runs_avgs = np.mean(est_is_best_over_t_runs, axis=0)\n",
    "plt_est_is_best_over_t_runs_avgs, = axs[1,0].plot(est_is_best_over_t_runs_avgs)\n",
    "\n",
    "# axs[1,0].legend()\n",
    "axs[1,0].set_xlabel(\"time step\")\n",
    "axs[1,0].set_ylabel(\"percentage\")\n",
    "axs[1,0].set_title(\"Percentage of Runs where Best Action was Chosen\", y=-0.18)\n",
    "\n",
    "#plot the mean instantaneous regret over time\n",
    "\n",
    "l_over_t_runs_avgs = np.mean(l_over_t_runs, axis=0)\n",
    "axs[1,1].plot(l_over_t_runs_avgs)\n",
    "\n",
    "# axs[1,1].legend()\n",
    "axs[1,1].set_xlabel(\"time step\")\n",
    "axs[1,1].set_ylabel(\"regret\")\n",
    "axs[1,1].set_title(\"Instantaneous Regret over Time\", y=-0.18)\n",
    "\n",
    "#plot the total regret over time\n",
    "\n",
    "total_l_over_t_runs_avgs = np.mean(total_l_over_t_runs, axis=0)\n",
    "axs[2,0].plot(total_l_over_t_runs_avgs)\n",
    "\n",
    "# axs[2,0].legend()\n",
    "axs[2,0].set_xlabel(\"time step\")\n",
    "axs[2,0].set_ylabel(\"regret\")\n",
    "axs[2,0].set_title(\"Total Regret up to Time Step t\", y=-0.18)\n",
    "\n",
    "axs[-1, -1].axis('off')\n",
    "\n",
    "title = r'Graphs for Thompson Sampling Bandit'\n",
    "fig.suptitle(title, fontsize=16, y=0.08)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRc45fxdc46B"
   },
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxB0EtT4MLnO"
   },
   "source": [
    "Thompson sampling seems to identify relatively quickly which arm is best, and then seems to largely stick to sampling that arm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggb7m7AwcFb1"
   },
   "source": [
    "## Q8 Comparison of Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMyZZ1P1PNqU"
   },
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "id": "RBo6IK0x6wBJ"
   },
   "source": [
    "eps_results = np.zeros((100, 5, 1000))\n",
    "grad_results = np.zeros((100, 5, 1000))\n",
    "thompson_results = np.zeros((100, 5, 1000))\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "  eps_results[i] = epsilon_greedy(three_arm_gaussian_bandit, 0.5, epsilon_decay = True, lambda_=0.1)\n",
    "  grad_results[i] = gradient_bandit(three_arm_gaussian_bandit, alpha=0.5, alpha_decay=True, lda=0.01, p=0.5)\n",
    "  thompson_results[i] = thompson_sampling(three_arm_gaussian_bandit)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "Ra6bXFWbXhKO"
   },
   "source": [
    "average_instantaneous_reward_over_time_epsilon = np.average(eps_results[:,0], axis=0)\n",
    "average_cumulative_reward_over_time_epsilon = np.average(eps_results[:,1], axis=0)\n",
    "average_estimated_best_action_over_time_epsilon = np.average(eps_results[:,2], axis=0)\n",
    "average_instantaneous_regret_over_time_epsilon = np.average(eps_results[:,3], axis=0)\n",
    "average_cumulative_regret_over_time_epsilon = np.average(eps_results[:,4], axis=0)\n",
    "\n",
    "average_instantaneous_reward_over_time_gradient = np.average(grad_results[:,0], axis=0)\n",
    "average_cumulative_reward_over_time_gradient = np.average(grad_results[:,1], axis=0)\n",
    "average_estimated_best_action_over_time_gradient = np.average(grad_results[:,2], axis=0)\n",
    "average_instantaneous_regret_over_time_gradient = np.average(grad_results[:,3], axis=0)\n",
    "average_cumulative_regret_over_time_gradient = np.average(grad_results[:,4], axis=0)\n",
    "\n",
    "average_instantaneous_reward_over_time_thompson = np.average(thompson_results[:,0], axis=0)\n",
    "average_cumulative_reward_over_time_thompson = np.average(thompson_results[:,1], axis=0)\n",
    "average_estimated_best_action_over_time_thompson = np.average(thompson_results[:,2], axis=0)\n",
    "average_instantaneous_regret_over_time_thompson = np.average(thompson_results[:,3], axis=0)\n",
    "average_cumulative_regret_over_time_thompson = np.average(thompson_results[:,4], axis=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ExxmIa6VY8rQ",
    "outputId": "b42094f1-0327-48d9-860b-69ba0d33d7ee"
   },
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(18, 18))\n",
    "\n",
    "axs[0, 0].plot(average_instantaneous_reward_over_time_epsilon, label = r\"$\\epsilon = decay$\")\n",
    "axs[0, 0].plot(average_instantaneous_reward_over_time_gradient, label = r\"$\\alpha = decay$, gradient\")\n",
    "axs[0, 0].plot(average_instantaneous_reward_over_time_thompson, label = \"thompson sampling\")\n",
    "axs[0, 0].set_xlabel(\"time step\")\n",
    "axs[0, 0].set_ylabel(\"reward value\")\n",
    "axs[0, 0].set_title(\"Average Instantaneous Reward Received over Time\", y=-0.18)\n",
    "axs[0, 0].legend()\n",
    "\n",
    "axs[0, 1].plot(average_cumulative_reward_over_time_epsilon, label = r\"$\\epsilon = decay$\")\n",
    "axs[0, 1].plot(average_cumulative_reward_over_time_gradient, label = r\"$\\alpha = decay$, gradient\")\n",
    "axs[0, 1].plot(average_cumulative_reward_over_time_thompson, label = \"thompson sampling\")\n",
    "axs[0, 1].set_xlabel(\"time step\")\n",
    "axs[0, 1].set_ylabel(\"reward value\")\n",
    "axs[0, 1].set_title(\"Average Cumulative Reward Received over Time\", y=-0.18)\n",
    "axs[0, 1].legend()\n",
    "\n",
    "axs[1, 0].plot(average_estimated_best_action_over_time_epsilon, label = r\"$\\epsilon = decay$\")\n",
    "axs[1, 0].plot(average_estimated_best_action_over_time_gradient, label = r\"$\\alpha = decay$, gradient\")\n",
    "axs[1, 0].plot(average_estimated_best_action_over_time_thompson, label = \"thompson sampling\")\n",
    "axs[1, 0].set_xlabel(\"time step\")\n",
    "axs[1, 0].set_ylabel(\"reward value\")\n",
    "axs[1, 0].set_title(\"% of time optimal action was chosen\", y=-0.18)\n",
    "axs[1, 0].legend()\n",
    "\n",
    "axs[1, 1].plot(average_instantaneous_regret_over_time_epsilon, label = r\"$\\epsilon = decay$\")\n",
    "axs[1, 1].plot(average_instantaneous_regret_over_time_gradient, label = r\"$\\alpha = decay$, gradient\")\n",
    "axs[1, 1].plot(average_instantaneous_regret_over_time_thompson, label = \"thompson sampling\")\n",
    "axs[1, 1].set_xlabel(\"time step\")\n",
    "axs[1, 1].set_ylabel(\"reward value\")\n",
    "axs[1, 1].set_title(\"Average Instantaneous Regret Received over Time\", y=-0.18)\n",
    "axs[1, 1].legend()\n",
    "\n",
    "axs[2, 0].plot(average_cumulative_regret_over_time_epsilon, label = r\"$\\epsilon = decay$\")\n",
    "axs[2, 0].plot(average_cumulative_regret_over_time_gradient, label = r\"$\\alpha = decay$, gradient\")\n",
    "axs[2, 0].plot(average_cumulative_regret_over_time_thompson, label = \"thompson sampling\")\n",
    "axs[2, 0].set_xlabel(\"time step\")\n",
    "axs[2, 0].set_ylabel(\"reward value\")\n",
    "axs[2, 0].set_title(\"Average Cumulative Regret Received over Time\", y=-0.18)\n",
    "axs[2, 0].legend()\n",
    "\n",
    "axs[-1, -1].axis('off')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wwel6MCoc8ms"
   },
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOWhRBqJP1nP"
   },
   "source": [
    "Decaying $\\epsilon$-greedy and gradient with decaying $\\alpha$ seems to perform very similarly. They both exhibit sublinear (i.e. logarithmic) growth in average cumulative regret, which is optimal, whereas Thompson sampling seems to exhibit roughly linear growth in average cumulative regret. However, Thompson sampling and gradient bandit both exhibit slower growth in average cumulative compared with $\\epsilon$-greedy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFBJd8hRHDbj"
   },
   "source": [
    "## Q9 Non-stationary Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "PbNLZxmiSLnP",
    "outputId": "0ff6a185-cb6d-4202-9053-50f03651e29b"
   },
   "source": [
    "delta = 0.2\n",
    "num_arms = 3\n",
    "means = np.array([0.5, 0.5+2*delta, 0.5-2*delta])\n",
    "variance = 0.01\n",
    "bandit = GaussianBandit(num_arms = num_arms, mean = means, variance = variance)\n",
    "station_changes = [500, bandit]\n",
    "\n",
    "# plot instantaneous\n",
    "\n",
    "avg_nonstationary_reward_epsilon_greedy_incremental, egi_cumu = epsilon_greedy(three_arm_gaussian_bandit, epsilon=0.25, stationary=False, nonstation_change=station_changes)[0:2]\n",
    "avg_nonstationary_reward_epsilon_greedy_lr, eglr_cumu = epsilon_greedy(three_arm_gaussian_bandit, epsilon=0.25, alpha=0.1, stationary=False, nonstation_change=station_changes)[0:2]\n",
    "avg_nonstationary_reward_epsilon_decay_incremental, egdi_cumu = epsilon_greedy(three_arm_gaussian_bandit, epsilon=0.5, epsilon_decay=True, lambda_=0.1, stationary=False, nonstation_change=station_changes)[0:2]\n",
    "avg_nonstationary_reward_epsilon_decay_lr, egdlr_cumu = epsilon_greedy(three_arm_gaussian_bandit, epsilon=0.5, epsilon_decay=True, lambda_=0.1, alpha=0.1, stationary=False, nonstation_change=station_changes)[0:2]\n",
    "\n",
    "avg_nonstationary_reward_gradient_01, grad01_cumu = gradient_bandit(three_arm_gaussian_bandit, alpha=0.1, stationary=False, nonstation_change=station_changes)[0:2]\n",
    "avg_nonstationary_reward_gradient_001, grad001_cumu = gradient_bandit(three_arm_gaussian_bandit, alpha=0.01, stationary=False, nonstation_change=station_changes)[0:2]\n",
    "\n",
    "avg_nonstationary_reward_thompson, thomp_cumu = thompson_sampling(three_arm_gaussian_bandit, stationary=False, nonstation_change=station_changes)[0:2]\n",
    "\n",
    "plt.plot(avg_nonstationary_reward_epsilon_greedy_incremental, label=\"epsilon=1/4, incremental averaging\")\n",
    "plt.plot(avg_nonstationary_reward_epsilon_greedy_lr, label=\"epsilon=1/4, alpha=0.1\")\n",
    "plt.plot(avg_nonstationary_reward_epsilon_decay_incremental, label=\"decaying epsilon, incremental averaging\")\n",
    "plt.plot(avg_nonstationary_reward_epsilon_decay_lr, label=\"decaying epsilon, alpha=0.1\")\n",
    "\n",
    "plt.plot(avg_nonstationary_reward_gradient_01, label=\"gradient bandit, alpha=0.1\")\n",
    "plt.plot(avg_nonstationary_reward_gradient_001, label=\"gradient bandit, alpha=0.01\")\n",
    "plt.plot(avg_nonstationary_reward_thompson, label=\"thompson sampling\")\n",
    "\n",
    "plt.xlabel(\"time step\")\n",
    "plt.ylabel(\"average instantaneous reward received\")\n",
    "\n",
    "# non_stat_greedy_rewards = epsilon_greedy(bandit, epsilon = 1/4, num_time_step = 1000, stationary=True)[0]\n",
    "# plt.plot(non_stat_greedy_rewards)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "moOcFg-4UkD9",
    "outputId": "9db54fe7-769c-440d-9206-9fd74b6cc98f"
   },
   "source": [
    "# plot cumulative rewards\n",
    "\n",
    "plt.plot(egi_cumu, label=\"epsilon=1/4, incremental averaging\")\n",
    "plt.plot(eglr_cumu, label=\"epsilon=1/4, alpha=0.1\")\n",
    "plt.plot(egdi_cumu, label=\"decaying epsilon, incremental averaging\")\n",
    "plt.plot(egdlr_cumu, label=\"decaying epsilon, alpha=0.1\")\n",
    "\n",
    "plt.plot(grad01_cumu, label=\"gradient bandit, alpha=0.1\")\n",
    "plt.plot(grad001_cumu, label=\"gradient bandit, alpha=0.01\")\n",
    "plt.plot(thomp_cumu, label=\"thompson sampling\")\n",
    "\n",
    "plt.xlabel(\"time step\")\n",
    "plt.ylabel(\"average reward received\")\n",
    "\n",
    "# non_stat_greedy_rewards = epsilon_greedy(bandit, epsilon = 1/4, num_time_step = 1000, stationary=True)[0]\n",
    "# plt.plot(non_stat_greedy_rewards)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcD9-XgZkHAn"
   },
   "source": [
    "By graphing the individual reward progressions for each method, we can observe how they respond to the change in reward distribution at $t=500$. $\\epsilon$-greedy with incremental averaging experiences a steep drop-off in average reward, and takes about $300$ iterations to identify the new best arm, after which it behaves as expected for a relatively high value of $\\epsilon$. On the other hand, $\\epsilon = \\frac{1}{4}$ with a fixed learning rate of $\\alpha = 0.1$ behaves about the same for the first $500$ iterations as in the incremental averaging case, but takes less than $50$ iterations after the distributions change to identify the new best arm. This suggests a fixed learning rate for $\\epsilon$-greedy is better suited to non-stationarity. In the decaying $\\epsilon$ case, the agent fails to identify the new best arm, and is stuck receiving sub-par rewards for the remaining iterations.\n",
    "\n",
    "Gradient bandit with $\\alpha = 0.1$ takes roughly 200 iterations to identify the new best arm after the distribution changes, and it does so slowly. Gradient bandit with $\\alpha = 0.01$ seemingly never identifies the new best arm, and experiences significant variation in average reward after $t=500$. Thompson sampling exhibits a sharp drop-off in average rewards, but around iteration 750 it identifies the new best arm, and largely samples that arm for the remainder of the runtime.\n",
    "\n",
    "In conclusion, Thompson sampling and $\\epsilon$-greedy with a fixed learning rate seem best adapted to non-stationary problems.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
