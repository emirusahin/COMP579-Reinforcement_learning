{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-21T20:56:48.098357Z",
     "start_time": "2025-03-21T20:56:47.729355Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import ale_py\n",
    "import os"
   ],
   "execution_count": 96,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T20:56:48.114943Z",
     "start_time": "2025-03-21T20:56:48.101367Z"
    }
   },
   "cell_type": "code",
   "source": "gym.register_envs(ale_py)",
   "id": "337a228a24186147",
   "execution_count": 97,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Neural Network Q-function Approximator",
   "id": "79e09d8606da0c5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T14:51:04.256314Z",
     "start_time": "2025-03-21T14:51:04.248293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256, n_layers=2):\n",
    "        super(QNetwork, self).__init__()\n",
    "        layers = []\n",
    "        last_dim = input_dim\n",
    "        # Create hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            layers.append(nn.Linear(last_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            last_dim = hidden_dim\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(last_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    #     self.initialize_weights()\n",
    "    # \n",
    "    # def initialize_weights(self):\n",
    "    #     # Initialize all Linear layers uniformly between -0.001 and 0.001\n",
    "    #     for m in self.model:\n",
    "    #         if isinstance(m, nn.Linear):\n",
    "    #             nn.init.xavier_uniform_(m.weight)\n",
    "    #             nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=1000000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (np.array(states),\n",
    "                np.array(actions),\n",
    "                np.array(rewards),\n",
    "                np.array(next_states),\n",
    "                np.array(dones))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# epsilon greedy action selection\n",
    "def epsilon_greedy_action(q_network, state, epsilon, n_actions):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(n_actions)\n",
    "    else:\n",
    "        state_tensor = torch.FloatTensor(np.array(state)).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_values = q_network(state_tensor)\n",
    "        return q_values.argmax().item()\n"
   ],
   "id": "3040f917c7337640",
   "execution_count": 75,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Compute expected q value for expexted sarsa",
   "id": "ce7e1371300df83d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T14:51:05.167382Z",
     "start_time": "2025-03-21T14:51:05.158613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_expected_q(q_network, next_state, epsilon, n_actions):\n",
    "    next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        q_values = q_network(next_state_tensor).squeeze(0)\n",
    "    # Determine the greedy action\n",
    "    max_action = q_values.argmax().item()\n",
    "    # Epsilon-greedy probabilities: probability for greedy action gets extra mass\n",
    "    probs = np.ones(n_actions) * (epsilon / n_actions)\n",
    "    probs[max_action] += (1 - epsilon)\n",
    "    expected_q = (probs * q_values.cpu().numpy()).sum()\n",
    "    return expected_q"
   ],
   "id": "da2053dfe32076f2",
   "execution_count": 76,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Single-episode training function that can handle training w/ or w/o replay_buffer",
   "id": "c75e790ff7cdb3df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T14:51:05.903815Z",
     "start_time": "2025-03-21T14:51:05.890650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_episode(env, q_network, optimizer, gamma, epsilon, algorithm, max_steps, replay_buffer=None, batch_size=32):\n",
    "    total_reward = 0.0\n",
    "    state,_ = env.reset()\n",
    "    done = False\n",
    "    steps = 0  # Step counter\n",
    "\n",
    "    while not done and steps < max_steps:\n",
    "        steps += 1\n",
    "        action = epsilon_greedy_action(q_network, state, epsilon, env.action_space.n)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "        # Update logic remains unchanged ...\n",
    "        if replay_buffer is None:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_val = q_network(state_tensor)[0, action] # gives the q value of the e-greedy action that we preciously selected\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(device) # convert next state array to tensor\n",
    "                with torch.no_grad():\n",
    "                    q_next = q_network(next_state_tensor) # predicts Q values for all action in the next state\n",
    "                if algorithm == 'q_learning':\n",
    "                    target = reward + gamma * q_next.max().item()  # gets the maximum out of those q values for the next state\n",
    "                elif algorithm == 'expected_sarsa':\n",
    "                    n_actions = env.action_space.n\n",
    "                    q_next = q_next.squeeze(0)\n",
    "                    max_action = q_next.argmax().item()\n",
    "                    probs = np.ones(n_actions) * (epsilon / n_actions)\n",
    "                    probs[max_action] += (1 - epsilon)\n",
    "                    expected_value = (probs * q_next.cpu().numpy()).sum()\n",
    "                    target = reward + gamma * expected_value\n",
    "                else:\n",
    "                    raise ValueError(\"Unknown algorithm\")\n",
    "            target = torch.tensor(target).to(device) # converts scalar value into tensor\n",
    "            loss = (q_val - target) ** 2\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "                states_tensor = torch.FloatTensor(states).to(device)\n",
    "                actions_tensor = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "                rewards_tensor = torch.FloatTensor(rewards).to(device)\n",
    "                next_states_tensor = torch.FloatTensor(next_states).to(device)\n",
    "                dones_tensor = torch.FloatTensor(dones).to(device)\n",
    "\n",
    "                q_values = q_network(states_tensor).gather(1, actions_tensor).squeeze(1)\n",
    "                with torch.no_grad():\n",
    "                    q_next = q_network(next_states_tensor)\n",
    "                if algorithm == 'q_learning':\n",
    "                    target_q = rewards_tensor + gamma * (1 - dones_tensor) * q_next.max(1)[0]\n",
    "                elif algorithm == 'expected_sarsa':\n",
    "                    n_actions = env.action_space.n\n",
    "                    expected_qs = []\n",
    "                    for i in range(batch_size):\n",
    "                        q_vals = q_next[i]\n",
    "                        max_action = q_vals.argmax().item()\n",
    "                        probs = np.ones(n_actions) * (epsilon / n_actions)\n",
    "                        probs[max_action] += (1 - epsilon)\n",
    "                        expected_qs.append((probs * q_vals.cpu().numpy()).sum())\n",
    "                    expected_qs = torch.FloatTensor(expected_qs).to(device)\n",
    "                    target_q = rewards_tensor + gamma * (1 - dones_tensor) * expected_qs\n",
    "                else:\n",
    "                    raise ValueError(\"Unknown algorithm\")\n",
    "                loss = nn.MSELoss()(q_values, target_q)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    return total_reward"
   ],
   "id": "fc36b3a22aba4649",
   "execution_count": 77,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Code for a single run (an experiment on only one seed)",
   "id": "666efdb01dae2ce6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T14:51:06.818527Z",
     "start_time": "2025-03-21T14:51:06.789136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_experiment(env_name, algorithm, use_replay, epsilon, lr, seed, episodes=1000, gamma=0.99, batch_size=32):\n",
    "    env = gym.make(env_name)\n",
    "    # Set maximum steps based on environment\n",
    "    if env_name == 'Acrobot-v1':\n",
    "        max_steps = 500\n",
    "    elif env_name == 'Assault-ram-v5':\n",
    "        max_steps = 1000\n",
    "    else:\n",
    "        max_steps = 500  # default or other environments\n",
    "\n",
    "    # Set seeds for reproducibility\n",
    "    state, info = env.reset(seed=seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    q_network = QNetwork(input_dim, output_dim, hidden_dim=256, n_layers=2).to(device)\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(capacity=1000000) if use_replay else None\n",
    "    episode_rewards = []\n",
    "    for ep in range(episodes):\n",
    "        total_reward = train_episode(env, q_network, optimizer, gamma, epsilon, algorithm, max_steps, replay_buffer, batch_size)\n",
    "        episode_rewards.append(total_reward)\n",
    "        print(f\"Episode: {ep}, Total Reward: {total_reward}\")\n",
    "    env.close()\n",
    "    return episode_rewards"
   ],
   "id": "9eed5c34cc4616b6",
   "execution_count": 78,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Code for all experiments ",
   "id": "e7612c903775800a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T14:51:07.547440Z",
     "start_time": "2025-03-21T14:51:07.530999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_multiple_trials(env_name, algorithm, use_replay, epsilon, lr, episodes=1000):\n",
    "    all_rewards = []\n",
    "    seeds = [1,2,3,4,5,6,7,8,9,10]\n",
    "    for seed in seeds:\n",
    "        print(f\"Seed {seed} ...\")\n",
    "        rewards = run_experiment(env_name, algorithm, use_replay, epsilon, lr, seed,\n",
    "                                 episodes=episodes)\n",
    "        all_rewards.append(rewards)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    mean_rewards = np.mean(all_rewards, axis=0)\n",
    "    std_rewards = np.std(all_rewards, axis=0)\n",
    "    return mean_rewards, std_rewards"
   ],
   "id": "40dc80dc04445ecd",
   "execution_count": 79,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Plotting for a single experiment on 10 seeds",
   "id": "b8dbe98a9f70adc6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T14:51:08.686299Z",
     "start_time": "2025-03-21T14:51:08.678251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_results(mean_rewards, std_rewards, episodes, title, filename):\n",
    "    plt.figure()\n",
    "    x = np.arange(episodes)\n",
    "    plt.plot(x, mean_rewards, label='Mean Reward')\n",
    "    plt.fill_between(x, mean_rewards - std_rewards, mean_rewards + std_rewards, alpha=0.2)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ],
   "id": "7f93c3275d818858",
   "execution_count": 80,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check how long a single run with 500 steps * 1000 episodes last for acrobot for Q_network and expected Sarsa",
   "id": "a59f71ab5c9bff38"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T15:04:50.700165Z",
     "start_time": "2025-03-21T14:54:23.603506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "# Example for Q-Learning on Acrobot\n",
    "start_time = time.time()\n",
    "rewards_q = run_experiment(\"Acrobot-v1\", \"q_learning\", use_replay=False,\n",
    "                           epsilon=0.1, lr=0.1, seed=0, episodes=1000, gamma=0.99, batch_size=32)\n",
    "elapsed_time_q = time.time() - start_time\n",
    "print(\"Q-Learning run time for Acrobot (max 500 steps/episode):\", elapsed_time_q, \"seconds\")"
   ],
   "id": "48910ded45a86e1b",
   "execution_count": 84,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T15:04:50.712498Z",
     "start_time": "2025-03-21T15:04:50.700165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# xavier_rewards = rewards_q (took 10 mins)\n",
    "kaming_default_rewards = rewards_q"
   ],
   "id": "46dfd1dfa39a47f9",
   "execution_count": 85,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T15:16:26.753549Z",
     "start_time": "2025-03-21T15:07:14.752134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example for Expected SARSA on Acrobot\n",
    "start_time = time.time()\n",
    "rewards_es = run_experiment(\"Acrobot-v1\", \"expected_sarsa\", use_replay=False,\n",
    "                            epsilon=0.1, lr=0.1, seed=0, episodes=1000, gamma=0.99, batch_size=32)\n",
    "elapsed_time_es = time.time() - start_time\n",
    "print(\"Expected SARSA run time for Acrobot (max 500 steps/episode):\", elapsed_time_es, \"seconds\")"
   ],
   "id": "835312715e765173",
   "execution_count": 87,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T15:27:17.696992Z",
     "start_time": "2025-03-21T15:27:17.425141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "episodes_range = range(1, 1001)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episodes_range, xavier_rewards, label=\"Xavier-Q-Learning\", color='green')\n",
    "plt.plot(episodes_range, kaming_default_rewards, label=\"default initialization_Qlearning\", color='red')\n",
    "plt.plot(episodes_range, rewards_es, label=\"Expected SARSA\", color='blue')\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Acrobot Reward Curves (Max 500 Steps per Episode)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "7d8a1ecafc4dfb13",
   "execution_count": 88,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:38:24.341754Z",
     "start_time": "2025-03-21T16:27:52.031098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "# Example for Q-Learning on Acrobot\n",
    "start_time = time.time()\n",
    "rewards_q = run_experiment(\"Acrobot-v1\", \"q_learning\", use_replay=False,\n",
    "                           epsilon=0.1, lr=0.01, seed=0, episodes=1000, gamma=0.99, batch_size=32)\n",
    "elapsed_time_q = time.time() - start_time\n",
    "print(\"Q-Learning run time for Acrobot (max 500 steps/episode):\", elapsed_time_q, \"seconds\")"
   ],
   "id": "833f55451dc1967f",
   "execution_count": 92,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:48:53.139452Z",
     "start_time": "2025-03-21T16:38:24.351181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "# Example for Q-Learning on Acrobot\n",
    "start_time = time.time()\n",
    "rewards_es = run_experiment(\"Acrobot-v1\", \"expected_sarsa\", use_replay=False,\n",
    "                           epsilon=0.1, lr=0.01, seed=0, episodes=1000, gamma=0.99, batch_size=32)\n",
    "elapsed_time_q = time.time() - start_time\n",
    "print(\"Q-Learning run time for Acrobot (max 500 steps/episode):\", elapsed_time_q, \"seconds\")"
   ],
   "id": "18588a670c4a06e5",
   "execution_count": 93,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:49:18.599231Z",
     "start_time": "2025-03-21T16:49:18.226497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "episodes_range = range(1, 1001)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episodes_range, rewards_q, label=\"Q-Learning\", color='green')\n",
    "plt.plot(episodes_range, rewards_es, label=\"Expected SARSA\", color='red')\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Acrobot Reward Curves (Max 500 Steps per Episode)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "3689fcfdb42a0059",
   "execution_count": 94,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:50:47.122433Z",
     "start_time": "2025-03-21T16:50:46.935463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def exponential_moving_average(data, alpha=0.1):\n",
    "    smoothed = []\n",
    "    smoothed.append(data[0])  # First value remains the same\n",
    "    for i in range(1, len(data)):\n",
    "        smoothed.append(alpha * data[i] + (1 - alpha) * smoothed[-1])\n",
    "    return smoothed\n",
    "\n",
    "alpha = 0.05  # Adjust smoothness factor\n",
    "\n",
    "smoothed_rewards_q = exponential_moving_average(rewards_q, alpha)\n",
    "smoothed_rewards_es = exponential_moving_average(rewards_es, alpha)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episodes_range, smoothed_rewards_q, label=\"Q-Learning (EMA Smoothed)\", color='green')\n",
    "plt.plot(episodes_range, smoothed_rewards_es, label=\"Expected SARSA (EMA Smoothed)\", color='red')\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Exponentially Smoothed Acrobot Reward Curves\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "7fbda1a0acca5718",
   "execution_count": 95,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Check how long it takes with replay buffer",
   "id": "b24d9a9eaf8b472c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T15:42:40.789329Z",
     "start_time": "2025-03-21T15:32:55.809366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "# Example for Q-Learning on Acrobot\n",
    "start_time = time.time()\n",
    "rewards_q = run_experiment(\"Acrobot-v1\", \"q_learning\", use_replay=True,\n",
    "                           epsilon=0.1, lr=0.1, seed=0, episodes=1000, gamma=0.99, batch_size=32)\n",
    "elapsed_time_q = time.time() - start_time\n",
    "print(\"Q-Learning run time for Acrobot (max 500 steps/episode):\", elapsed_time_q, \"seconds\")"
   ],
   "id": "df3eb3f34ff2544",
   "execution_count": 89,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T15:53:37.017322Z",
     "start_time": "2025-03-21T15:42:40.795824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example for Expected SARSA on Acrobot\n",
    "start_time = time.time()\n",
    "rewards_es = run_experiment(\"Acrobot-v1\", \"expected_sarsa\", use_replay=True,\n",
    "                            epsilon=0.1, lr=0.1, seed=0, episodes=1000, gamma=0.99, batch_size=32)\n",
    "elapsed_time_es = time.time() - start_time\n",
    "print(\"Expected SARSA run time for Acrobot (max 500 steps/episode):\", elapsed_time_es, \"seconds\")"
   ],
   "id": "dbfa3b367f5fc9c0",
   "execution_count": 90,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T15:53:37.301112Z",
     "start_time": "2025-03-21T15:53:37.020773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "episodes_range = range(1, 1001)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episodes_range, rewards_q, label=\"Q-Learning\", color='green')\n",
    "plt.plot(episodes_range, rewards_es, label=\"Expected SARSA\", color='red')\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Acrobot Reward Curves (Max 500 Steps per Episode)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "d97c06d6b9f662b",
   "execution_count": 91,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run all experiments",
   "id": "962fe35919a5d86e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-21T05:58:10.033657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Create a directory to save plots if it doesn't exist\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "# List of environments\n",
    "env_list = ['Acrobot-v1', 'Assault-ram-v5']\n",
    "#  epsilon values \n",
    "epsilon_list = [0.05, 0.1, 0.15]\n",
    "# Step-size parameters \n",
    "lr_list = [0.1, 0.01, 0.001]\n",
    "# Algorithms to compare\n",
    "algorithms = ['q_learning', 'expected_sarsa']\n",
    "# Whether to use replay buffer or not\n",
    "replay_options = [False, True]\n",
    "\n",
    "num_seeds = 10      # 10 learning trials\n",
    "episodes = 1000     # 1000 episodes per run\n",
    "gamma = 0.99        # Discount factor\n",
    "\n",
    "# Loop over each environment\n",
    "for env_name in env_list:\n",
    "    # For each replay configuration\n",
    "    for use_replay in replay_options:\n",
    "        # For each combination of epsilon and step-size (lr)\n",
    "        for epsilon in epsilon_list:\n",
    "            for lr in lr_list:\n",
    "                # For each algorithm: Q-learning (green) and Expected SARSA (red)\n",
    "                for algo in algorithms:\n",
    "                    title = (f\"{env_name} - {algo.upper()} - \" +\n",
    "                             f\"{'Replay' if use_replay else 'No Replay'} - \" +\n",
    "                             f\"Ïµ: {epsilon}, LR: {lr}\")\n",
    "                    print(\"Running:\", title)\n",
    "                    mean_rewards, std_rewards = run_multiple_trials(env_name, algo,\n",
    "                                                                    use_replay, epsilon,\n",
    "                                                                    lr,\n",
    "                                                                    episodes)\n",
    "                    # Save the plot\n",
    "                    fname = f\"plots/{env_name}_{algo}_{'replay' if use_replay else 'no_replay'}_eps{epsilon}_lr{lr}.png\"\n",
    "                    plot_results(mean_rewards, std_rewards, episodes, title, fname)\n",
    "                    print(f\"Saved plot to {fname}\")"
   ],
   "id": "fc4f67cca0f99b16",
   "execution_count": 51,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "eebb66d8e2f54830",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
