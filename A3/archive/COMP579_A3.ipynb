{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-20T02:19:04.943261Z",
     "start_time": "2025-03-20T02:19:04.938264Z"
    }
   },
   "source": [
    "# TODO: Set up the environments and check if they work out\n",
    "# TODO: Create DQN algorithm as a class\n",
    "# TODO: Train it for epsilon=0.1, step_size=0.001, 1 seed for 1000 episodes, save the performance and the model\n",
    "# TODO: \n",
    "# TODO: \n",
    "# TODO: Create Expected Sarsa (look up previous assignment)\n",
    "# TODO: "
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:09:39.850297Z",
     "start_time": "2025-03-20T20:07:22.790475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install gymnasium\n",
    "!pip install ale-py\n",
    "!pip install gymnasium[classic-control]\n",
    "!pip install torch"
   ],
   "id": "1e5f953a5a541d64",
   "execution_count": 69,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T01:44:06.840893Z",
     "start_time": "2025-03-21T01:43:59.456170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque"
   ],
   "id": "36c83def1a437be8",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T01:44:06.983153Z",
     "start_time": "2025-03-21T01:44:06.841991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gym.register_envs(ale_py)\n",
    "\n",
    "envs = {\n",
    "    \"acrobot\": gym.make('Acrobot-v1', render_mode=\"rgb_array\"),\n",
    "    \"assault\": gym.make('ALE/Assault-ram-v5', render_mode=\"rgb_array\")\n",
    "}"
   ],
   "id": "f0811260e1afd7ca",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T01:44:06.992950Z",
     "start_time": "2025-03-21T01:44:06.985184Z"
    }
   },
   "cell_type": "code",
   "source": "envs[\"acrobot\"]",
   "id": "6e2ee42beddd2325",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T01:44:07.006961Z",
     "start_time": "2025-03-21T01:44:06.995057Z"
    }
   },
   "cell_type": "code",
   "source": "envs[\"acrobot\"].reset(seed=123, options={\"low\": -0.2, \"high\": 0.2})",
   "id": "db357bce58bbd530",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T01:44:07.017643Z",
     "start_time": "2025-03-21T01:44:07.007966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for env_name,env in envs.items():\n",
    "    print(f\"Environment: {env_name}\")\n",
    "    print(f\"State Space: {env.observation_space}\")\n",
    "    # print(f\"State Space Type: {type(env.observation_space)}\")\n",
    "    print(\"Action space:\", env.action_space)\n",
    "    # print(f\"Action Space Type: {type(env.action_space)}\")"
   ],
   "id": "a32ade0283bfda80",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T01:44:07.034313Z",
     "start_time": "2025-03-21T01:44:07.018656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "# Neural Network (MLP) for Function Approximation\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                nn.init.constant_(m.bias, 0.01)  #  initialize biases to 0.1\n",
    "\n",
    "    def forward(self, x):\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            return self.fc3(x)\n",
    "\n",
    "# Experience Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.tensor(states, dtype=torch.float),\n",
    "            torch.tensor(actions, dtype=torch.int64),\n",
    "            torch.tensor(rewards, dtype=torch.float),\n",
    "            torch.tensor(next_states, dtype=torch.float),\n",
    "            torch.tensor(dones, dtype=torch.bool),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# DQN Agent\n",
    "class DQN:\n",
    "    def __init__(self, state_dim, action_dim, replay_buffer=True, learning_rate=1e-3, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.replay_buffer = replay_buffer  # True -> Uses Replay Buffer, False -> Learns from latest transition\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = 32\n",
    "        self.target_update_freq = 10\n",
    "\n",
    "        # Q-network and target network\n",
    "        self.q_network = MLP(state_dim, action_dim)\n",
    "        self.target_network = MLP(state_dim, action_dim)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())  # Copy initial weights\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        # Initialize replay buffer if needed\n",
    "        self.buffer = ReplayBuffer(capacity=10000) if self.replay_buffer else None\n",
    "\n",
    "        # If no replay buffer, store the last transition\n",
    "        self.last_transition = None\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                return torch.argmax(self.q_network(state)).item()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the agent using a batch of experiences from replay buffer.\"\"\"\n",
    "        if self.replay_buffer:\n",
    "            if len(self.buffer) < self.batch_size:\n",
    "                return  # Wait until we have enough data\n",
    "    \n",
    "            # Sample a batch from replay buffer\n",
    "            states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "        else:\n",
    "            if self.last_transition is None:\n",
    "                return\n",
    "    \n",
    "            # Convert single transition to a batch of size 1\n",
    "            states, actions, rewards, next_states, dones = self.last_transition\n",
    "            states, actions, rewards, next_states, dones = (\n",
    "                states.unsqueeze(0),\n",
    "                actions.unsqueeze(0),\n",
    "                rewards.unsqueeze(0),\n",
    "                next_states.unsqueeze(0),\n",
    "                dones.unsqueeze(0),\n",
    "            )\n",
    "    \n",
    "        # Compute Q-values for selected actions\n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + self.gamma * next_q_values * (~dones)\n",
    "    \n",
    "        # Compute loss (Mean Squared Error)\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "    \n",
    "        # Optimize the network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights from Q-network to target network.\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n"
   ],
   "id": "9ab01f98a5bcc628",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T02:54:13.082349Z",
     "start_time": "2025-03-21T02:54:13.010646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "# Neural Network (MLP) for Function Approximation\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.uniform_(m.weight, -0.01, 0.01)  # Uniform distribution [-0.01, 0.01]\n",
    "                nn.init.constant_(m.bias, 0.01)  # Initialize biases to 0.01\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Experience Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.tensor(states, dtype=torch.float),\n",
    "            torch.tensor(actions, dtype=torch.int64),\n",
    "            torch.tensor(rewards, dtype=torch.float),\n",
    "            torch.tensor(next_states, dtype=torch.float),\n",
    "            torch.tensor(dones, dtype=torch.bool),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# DQN Agent\n",
    "class DQN:\n",
    "    def __init__(self, state_dim, action_dim, replay_buffer=True, learning_rate=1e-3, gamma=0.99,\n",
    "                 epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.replay_buffer = replay_buffer  # True -> Uses Replay Buffer, False -> Learns from latest transition\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = 64\n",
    "        self.target_update_freq = 10\n",
    "\n",
    "        # Q-network and target network\n",
    "        self.q_network = MLP(state_dim, action_dim)\n",
    "        self.target_network = MLP(state_dim, action_dim)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())  # Copy initial weights\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        # Initialize replay buffer if needed\n",
    "        self.buffer = ReplayBuffer(capacity=10000) if self.replay_buffer else None\n",
    "\n",
    "        # If no replay buffer, store the last transition\n",
    "        self.last_transition = None\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                return torch.argmax(self.q_network(state)).item()\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Stores transition in replay buffer or keeps the latest one.\"\"\"\n",
    "        if self.replay_buffer:\n",
    "            self.buffer.push(state, action, reward, next_state, done)\n",
    "        else:\n",
    "            self.last_transition = (\n",
    "                torch.tensor([state], dtype=torch.float),\n",
    "                torch.tensor([action], dtype=torch.int64),\n",
    "                torch.tensor([reward], dtype=torch.float),\n",
    "                torch.tensor([next_state], dtype=torch.float),\n",
    "                torch.tensor([done], dtype=torch.bool),\n",
    "            )\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the agent using replay buffer or latest transition.\"\"\"\n",
    "        if self.replay_buffer:\n",
    "            # Ensure there is enough data in the buffer\n",
    "            if len(self.buffer) < self.batch_size:\n",
    "                return\n",
    "\n",
    "            # Sample batch from replay buffer\n",
    "            states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "        else:\n",
    "            # Train only on the last transition (No replay buffer)\n",
    "            if self.last_transition is None:\n",
    "                return\n",
    "            states, actions, rewards, next_states, dones = self.last_transition\n",
    "\n",
    "        # Compute Q-values\n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.target_network(next_states).max(1)[0].detach()\n",
    "        target_q_values = rewards + self.gamma * next_q_values * (~dones)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "\n",
    "        # Optimize model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights from Q-network to target network.\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n"
   ],
   "id": "b018267148a6c2cb",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T02:04:12.732314Z",
     "start_time": "2025-03-21T01:44:07.036333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = envs[\"acrobot\"]\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Set replay_buffer=True to use replay buffer, False to disable it\n",
    "agent = DQN(state_dim, action_dim, replay_buffer=True)\n",
    "\n",
    "num_episodes = 1000\n",
    "reward_history = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(500):  # Max steps per episode 500 for acrobot, 1000 for assult\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if agent.replay_buffer:\n",
    "            agent.buffer.push(state, action, reward, next_state, done)\n",
    "        else:\n",
    "            # Store the last transition in a single tuple\n",
    "            agent.last_transition = (\n",
    "                torch.tensor([state], dtype=torch.float),\n",
    "                torch.tensor([action], dtype=torch.int64),\n",
    "                torch.tensor([reward], dtype=torch.float),\n",
    "                torch.tensor([next_state], dtype=torch.float),\n",
    "                torch.tensor([done], dtype=torch.bool),\n",
    "            )\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        # Train the agent\n",
    "        agent.train()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update target network every few episodes\n",
    "    if episode % agent.target_update_freq == 0:\n",
    "        agent.update_target_network()\n",
    "\n",
    "    # Decay epsilon\n",
    "    agent.decay_epsilon()\n",
    "\n",
    "    reward_history.append(total_reward)\n",
    "    print(f\"Episode {episode}, Reward: {total_reward}, Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "env.close()\n"
   ],
   "id": "52ef723de82a0dd4",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T02:04:13.152419Z",
     "start_time": "2025-03-21T02:04:12.753635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.plot(reward_history)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"DQN Training Performance\")\n",
    "plt.show()\n"
   ],
   "id": "1b1f359d57499d2a",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T02:24:19.877585Z",
     "start_time": "2025-03-21T02:24:13.970640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = gym.make(\"Acrobot-v1\", render_mode=\"human\")\n",
    "\n",
    "state, _ = env.reset()\n",
    "for _ in range(500):\n",
    "    action = agent.select_action(state)\n",
    "    state, _, terminated, truncated, _ = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()\n"
   ],
   "id": "84dd7ca7eea744ce",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T03:31:03.490942Z",
     "start_time": "2025-03-21T02:55:39.192230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = envs[\"acrobot\"]\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Set replay_buffer=True to use replay buffer, False to disable it\n",
    "agent = DQN(state_dim, action_dim, replay_buffer=False)\n",
    "\n",
    "num_episodes = 1000\n",
    "reward_history_no_replay = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(500):  # Max steps per episode 500 for acrobot, 1000 for assult\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if agent.replay_buffer:\n",
    "            agent.buffer.push(state, action, reward, next_state, done)\n",
    "        else:\n",
    "            # Store the last transition in a single tuple\n",
    "            agent.last_transition = (\n",
    "                torch.tensor([state], dtype=torch.float),\n",
    "                torch.tensor([action], dtype=torch.int64),\n",
    "                torch.tensor([reward], dtype=torch.float),\n",
    "                torch.tensor([next_state], dtype=torch.float),\n",
    "                torch.tensor([done], dtype=torch.bool),\n",
    "            )\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        # Train the agent\n",
    "        agent.train()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update target network every few episodes\n",
    "    if episode % agent.target_update_freq == 0:\n",
    "        agent.update_target_network()\n",
    "\n",
    "    # Decay epsilon\n",
    "    agent.decay_epsilon()\n",
    "\n",
    "    reward_history_no_replay.append(total_reward)\n",
    "    print(f\"Episode {episode}, Reward: {total_reward}, Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "env.close()\n"
   ],
   "id": "f457aa0758e17478",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T03:40:54.857103Z",
     "start_time": "2025-03-21T03:40:54.449311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.plot(reward_history_no_replay)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"DQN Training Performance\")\n",
    "plt.show()\n"
   ],
   "id": "e5dfdc090574c9af",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T03:41:30.172367Z",
     "start_time": "2025-03-21T03:40:56.478515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = gym.make(\"Acrobot-v1\", render_mode=\"human\")\n",
    "\n",
    "state, _ = env.reset()\n",
    "for _ in range(500):\n",
    "    action = agent.select_action(state)\n",
    "    state, _, terminated, truncated, _ = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()\n"
   ],
   "id": "5362d3881da1f0b3",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "10a354d95529ef3a",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
