{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKkeYE20a37b"
   },
   "source": [
    "# COMP 579 Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Do Not Change the Random Seed\n",
    "The random seed has been set to ensure reproducibility. Please do not modify it.\n",
    "\n",
    "2. Guidance for the First Question\n",
    "For the initial question, fill in the blanks under the sections marked as TODO. Follow the provided structure and complete the missing parts.\n",
    "\n",
    "3. Approach for Subsequent Questions\n",
    "For the later questions, we expect you to attempt the solutions independently. You can refer to the examples provided in earlier questions to understand how to \n",
    "plot figures and implement solutions.\n",
    "\n",
    "4. Ensure that the plots you produce for later questions are similar in style and format to those shown in the previous examples."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eEvd8WcFqvai",
    "ExecuteTime": {
     "end_time": "2025-01-30T19:24:56.219627Z",
     "start_time": "2025-01-30T19:24:55.296950Z"
    }
   },
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from IPython.core.debugger import set_trace\n",
    "np.random.seed(40)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"]=10,5"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ss77N5TbLVIl"
   },
   "source": [
    "## Q1 Simulator for Bernoulli Bandit"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uG8suY4Sn7hu",
    "ExecuteTime": {
     "end_time": "2025-01-30T19:24:56.226502Z",
     "start_time": "2025-01-30T19:24:56.221644Z"
    }
   },
   "source": [
    "class GaussianBandit:\n",
    "  \"\"\"\n",
    "    A class representing a Gaussian multi-armed bandit.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    num_arms : int\n",
    "        Number of arms in the bandit.\n",
    "    mean : list or np.ndarray\n",
    "        List of mean rewards for each arm.\n",
    "    variance : float\n",
    "        Variance of the rewards for all arms.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    sample(arm_index)\n",
    "        Samples a reward from the specified arm based on a Gaussian distribution.\n",
    "    \"\"\"\n",
    "\n",
    "  # TODO:\n",
    "  def __init__(self, num_arms, mean, variance):\n",
    "    self.num_arms = num_arms\n",
    "    self.mean = mean\n",
    "    self.variance = variance\n",
    "\n",
    "  def sample(self, arm_index):\n",
    "    chosen_mean = self.mean[arm_index]\n",
    "    return np.random.normal(chosen_mean,np.sqrt(self.variance))\n",
    " "
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "A2G0G_s5sy_C",
    "ExecuteTime": {
     "end_time": "2025-01-30T19:24:56.241469Z",
     "start_time": "2025-01-30T19:24:56.226502Z"
    }
   },
   "source": [
    "# TODO:\n",
    "delta = 0.2\n",
    "num_arms = 3\n",
    "mean = [0.5,0.5-delta,0.5+delta]\n",
    "variance = 0.01\n",
    "num_samples = 50\n",
    "\n",
    "three_arm_gaussian_bandit = GaussianBandit(num_arms,mean,variance)\n",
    "\n",
    "# Store the rewards for each arm\n",
    "action_rewards = []\n",
    "actions = range(num_arms)\n",
    "\n",
    "for action in actions:\n",
    "    # Store 50 samples per action\n",
    "    rewards = []\n",
    "    for i in range(num_samples): # Will sample 50 times for the same action\n",
    "       sample_reward = three_arm_gaussian_bandit.sample(action)\n",
    "       rewards.append(sample_reward)\n",
    "        \n",
    "    action_rewards.append(rewards)\n"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UG_coTYL1_RL"
   },
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pj04OZ0ozUjK",
    "outputId": "ee09b4df-c2f2-4d98-8146-0c2e184b64c7",
    "ExecuteTime": {
     "end_time": "2025-01-30T19:24:56.694692Z",
     "start_time": "2025-01-30T19:24:56.242869Z"
    }
   },
   "source": [
    "for action in actions:\n",
    "  fig, ax = plt.subplots()\n",
    "\n",
    "  # TODO:\n",
    "  true_value = three_arm_gaussian_bandit.mean[action]\n",
    "  estimated_value = np.mean(action_rewards[action])\n",
    "\n",
    "  # draw the line of the true value\n",
    "  line_true_val = ax.axhline(y = true_value, color = 'b', linestyle = ':', label = \"true value, q*\")\n",
    "  # draw the line of the estimated value\n",
    "  line_est_val = ax.axhline(y = estimated_value, color = 'r', linestyle = '--', label = \"estimated value, q\")\n",
    "  # plot the reward samples\n",
    "  plt_samples, = ax.plot(action_rewards[action], 'o', label = \"reward samples\")\n",
    "\n",
    "  ax.set_xlabel(\"sample number\")\n",
    "  ax.set_ylabel(\"reward value\")\n",
    "  ax.set_title(\"Sample reward, estimated and true expected reward over 50 samples for action %s\" %action, y=-0.2)\n",
    "\n",
    "  # show the legend with the labels of the line\n",
    "  ax.legend(handles=[line_true_val, line_est_val, plt_samples])"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKLg6U5bLhRs"
   },
   "source": [
    "## Q2 Estimated Q values"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cgnKe19V0NgR",
    "ExecuteTime": {
     "end_time": "2025-01-30T19:24:56.703015Z",
     "start_time": "2025-01-30T19:24:56.696699Z"
    }
   },
   "source": [
    "def update(reward_samples, alpha):\n",
    "  \"\"\"\n",
    "  Each call to the function yields the current incremental average of the reward with a fixed learning rate, alpha\n",
    "  E.g. Initial call returns alpha * reward_samples[0], second call returns prev_val + alpha * (reward_samples[1] - prev_val)\n",
    "  where prev_val is the value return from the previous call, so on and so forth\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  reward_samples : array of int\n",
    "      samples of reward values from one arm of a bandit\n",
    "  alpha : int\n",
    "      learning rate parameter for the averaging\n",
    "  \"\"\"\n",
    "  incremental_avg = 0 \n",
    "  for reward in reward_samples:\n",
    "    incremental_avg = incremental_avg + alpha * (reward - incremental_avg)\n",
    "    yield incremental_avg\n",
    "\n",
    "def updateAvg(reward_samples):\n",
    "  \"\"\"\n",
    "  Each call to the function yields the current incremental average of the reward\n",
    "  E.g. Inital call returns reward_samples[0], second call returns the average of reward_samples[0] and reward_samples[1], so on and so forth\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  reward_samples : array of int\n",
    "      samples of reward values from one arm of a bandit\n",
    "  \"\"\"\n",
    "  rewards = []\n",
    "  for reward in reward_samples:\n",
    "    rewards.append(reward)\n",
    "    yield np.mean(rewards) # not the optimal way you can use the derivation of incremental update formula\n",
    "    \n",
    "def updateDecaying(reward_samples, alpha_0=0.5, lambda_=0.01, p=0.5):\n",
    "    \"\"\"\n",
    "    Each call to the function yields the updated estimate of the action value using an\n",
    "    improved decaying learning rate.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reward_samples : array-like of int or float\n",
    "        Samples of reward values from one arm of a bandit.\n",
    "    alpha_0 : float, optional\n",
    "        The initial learning rate (default is 0.5).\n",
    "    lambda_ : float, optional\n",
    "        The decay rate constant (default is 0.01).\n",
    "    p : float, optional\n",
    "        The power parameter for controlling decay (default is 0.5).\n",
    "    \"\"\"\n",
    "    \n",
    "    t = 0\n",
    "    incremental_avg = 0\n",
    "    for reward in reward_samples:\n",
    "      alpha_t = alpha_0/((1+lambda_*t)**p)\n",
    "      incremental_avg = incremental_avg + alpha_t * (reward - incremental_avg)\n",
    "      t += 1\n",
    "      yield incremental_avg"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKmG74R11x-j"
   },
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "G_ltKRTcBaDM",
    "outputId": "5e979ea2-7cb2-4978-856d-71a3899b106e",
    "ExecuteTime": {
     "end_time": "2025-01-30T19:24:57.757658Z",
     "start_time": "2025-01-30T19:24:57.093404Z"
    }
   },
   "source": [
    "for action in actions:\n",
    "  fig, ax = plt.subplots()\n",
    "\n",
    "  # TODO:\n",
    "  incr_avgs = list(updateAvg(action_rewards[action])) # a list showing the estimated value function when you use incremental average function after each reward\n",
    "  alpha_1_percent = list(update(action_rewards[action],alpha=0.01)) # a list showing estimated value function when you use learning rate alpha to update the estimate after each reward\n",
    "  alpha_10_percent = list(update(action_rewards[action],alpha=0.1)) # same as the above one\n",
    "  alpha_decay = list(updateDecaying(action_rewards[action],alpha_0=0.5, lambda_=0.01, p=0.5)) # # a list showing estimated value function when you use high learning rate which gets smaller after each reward\n",
    "  true_value = mean[action]\n",
    "\n",
    "  # draw the true value line\n",
    "  line_true_val = ax.axhline(y = true_value, color = 'b', linestyle = ':', label = \"true value\")\n",
    "\n",
    "  # plot incremental values for averaging, alpha = 0.01, alpha = 0.1\n",
    "  plt_incr_avgs, = ax.plot(incr_avgs, label = \"incremental average\")\n",
    "  plt_alpha_1_percent, = ax.plot(alpha_1_percent, label = r\"$\\alpha = 0.01$\")\n",
    "  plt_alpha_10_percent, = ax.plot(alpha_10_percent, label = r\"$\\alpha = 0.1$\")\n",
    "  plt_alpha_decay, = ax.plot(alpha_decay, label = r\"$\\alpha decay$\")\n",
    "\n",
    "  ax.set_xlabel(\"sample number\")\n",
    "  ax.set_ylabel(\"reward value\")\n",
    "  ax.set_title(\"Incremental estimates and true expected reward values over 50 samples for action %s\" %action, y=-0.2)\n",
    "\n",
    "  # show the legend with the labels of the line\n",
    "  ax.legend(handles=[line_true_val, plt_incr_avgs, plt_alpha_1_percent, plt_alpha_10_percent, plt_alpha_decay])"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We see that alpha = 0.01 is very slow to improve its estimated Q value"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMdCH4-lLw44"
   },
   "source": [
    "## Q3 Effect of $α$ on Estimated Q values"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-6_tT59BB1Ro",
    "ExecuteTime": {
     "end_time": "2025-01-30T19:24:58.096635Z",
     "start_time": "2025-01-30T19:24:57.757658Z"
    }
   },
   "source": [
    "# TODO:\n",
    "num_samples = 100\n",
    "\n",
    "# arrays of the data generated from 100 runs\n",
    "incr_avgs_runs = []\n",
    "alpha_1_percent_runs = []\n",
    "alpha_10_percent_runs = []\n",
    "alpha_decay_runs = []\n",
    "\n",
    "\n",
    "# TODO:\n",
    "for run in range(100):\n",
    "  # arrays of data generated from the 3 actions in 1 run\n",
    "  sample_incr_avgs_by_actions = []\n",
    "  sample_alpha_1_percent_by_actions = []\n",
    "  sample_alpha_10_percent_by_actions = []\n",
    "  sample_alpha_decay_by_actions = []\n",
    "\n",
    "  for action in actions:\n",
    "    \n",
    "    # rewards = [0] USE THIS IF YOU WANNA HAVE r0 for all runs = 0 \n",
    "    rewards = []\n",
    "    for i in range(num_samples): # Will sample 100 times for the same action\n",
    "        rewards.append(three_arm_gaussian_bandit.sample(action))\n",
    "\n",
    "    sample_incr_avgs_by_actions.append(list(updateAvg(rewards)))\n",
    "    sample_alpha_1_percent_by_actions.append(list(update(rewards,alpha=0.01)))\n",
    "    sample_alpha_10_percent_by_actions.append(list(update(rewards,alpha=0.1)))\n",
    "    sample_alpha_decay_by_actions.append(list(updateDecaying(rewards,alpha_0=0.5, lambda_=0.01, p=0.5)))\n",
    "\n",
    "  incr_avgs_runs.append(sample_incr_avgs_by_actions)\n",
    "  alpha_1_percent_runs.append(sample_alpha_1_percent_by_actions)\n",
    "  alpha_10_percent_runs.append(sample_alpha_10_percent_by_actions)\n",
    "  alpha_decay_runs.append(sample_alpha_decay_by_actions)\n",
    "\n",
    "print(incr_avgs_runs)\n",
    "# convert to np arrays\n",
    "incr_avgs_runs = np.asarray(incr_avgs_runs)\n",
    "alpha_1_percent_runs = np.asarray(alpha_1_percent_runs)\n",
    "alpha_10_percent_runs = np.asarray(alpha_10_percent_runs)\n",
    "alpha_decay_runs = np.asarray(alpha_decay_runs)"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T19:24:58.118902Z",
     "start_time": "2025-01-30T19:24:58.112283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Debugging\n",
    "print(incr_avgs_runs[0])\n",
    "print(len(incr_avgs_runs))\n",
    "print(len(incr_avgs_runs[0]))\n",
    "print(len(incr_avgs_runs[0][0]))"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T19:24:58.240974Z",
     "start_time": "2025-01-30T19:24:58.225982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def list_of_ith_rewards_over_runs(runs, action):\n",
    "  ith_rewards_over_jth_run = []\n",
    "\n",
    "  for i in range(len(runs)):  # 0 to 99 # len(runs) = number of trials , if you initialize the first r = 0, this wont be enough since your len(run[action]) will be 101\n",
    "    ith_rewards = []\n",
    "    for run in runs:  # for each run\n",
    "      ith_rewards.append(run[action][i])\n",
    "    ith_rewards_over_jth_run.append(ith_rewards)\n",
    "\n",
    "  return ith_rewards_over_jth_run\n",
    "\n",
    "# Run function and check the length\n",
    "list_of_lists = list_of_ith_rewards_over_runs(incr_avgs_runs, 0)\n",
    "print(len(list_of_lists))  # Should be 100\n",
    "print(len(list_of_lists[0])) # should be 101 since we set the reward0 = 0"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaDSMygu2IZc"
   },
   "source": [
    "### Graphs"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T19:25:00.595127Z",
     "start_time": "2025-01-30T19:24:59.709354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for action in actions:\n",
    "  fig, ax = plt.subplots()\n",
    "\n",
    "  # obtain averaged incremental reward values for averaging, alpha = 0.01, alpha = 0.1 and decay alpha over 100 runs\n",
    "  # TODO:\n",
    "  ithrewards_incr_avgs_by_actions = list_of_ith_rewards_over_runs(incr_avgs_runs, action)\n",
    "  ithrewards_alpha_1_percent_by_actions = list_of_ith_rewards_over_runs(alpha_1_percent_runs, action)\n",
    "  ithrewards_alpha_10_percent_by_actions = list_of_ith_rewards_over_runs(alpha_10_percent_runs, action)\n",
    "  ithrewards_alpha_decay_by_actions = list_of_ith_rewards_over_runs(alpha_decay_runs, action)\n",
    "  \n",
    "  mean_incr_avgs_by_actions = np.array([np.mean(rewards) for rewards in ithrewards_incr_avgs_by_actions])\n",
    "  mean_alpha_1_percent_by_actions = np.array([np.mean(rewards) for rewards in ithrewards_alpha_1_percent_by_actions])\n",
    "  mean_alpha_10_percent_by_actions = np.array([np.mean(rewards) for rewards in ithrewards_alpha_10_percent_by_actions])\n",
    "  mean_alpha_decay_by_actions = np.array([np.mean(rewards) for rewards in ithrewards_alpha_decay_by_actions])\n",
    "\n",
    "  true_value = mean[action]\n",
    "\n",
    "  # obtain the standard deviation for averaging, alpha = 0.01, alpha = 0.1 and decay alpha over 100 runs\n",
    "  std_incr_avgs_by_actions = np.array([np.std(rewards) for rewards in ithrewards_incr_avgs_by_actions])\n",
    "  std_alpha_1_percent_by_actions = np.array([np.std(rewards) for rewards in ithrewards_alpha_1_percent_by_actions])\n",
    "  std_alpha_10_percent_by_actions = np.array([np.std(rewards) for rewards in ithrewards_alpha_10_percent_by_actions])\n",
    "  std_alpha_decay_by_actions =  np.array([np.std(rewards) for rewards in ithrewards_alpha_decay_by_actions])\n",
    "\n",
    "  # obtain the standard error for averaging, alpha = 0.01, alpha = 0.1 and decay alpha over 100 runs\n",
    "  std_err_incr_avgs_by_actions = np.array([sd / np.sqrt(num_samples) for sd in std_incr_avgs_by_actions])\n",
    "  std_err_alpha_1_percent_by_actions = np.array([sd / np.sqrt(num_samples) for sd in std_alpha_1_percent_by_actions])\n",
    "  std_err_alpha_10_percent_by_actions = np.array([sd / np.sqrt(num_samples) for sd in std_alpha_10_percent_by_actions])\n",
    "  std_err_alpha_decay_by_actions = np.array([sd / np.sqrt(num_samples) for sd in std_alpha_decay_by_actions])\n",
    "\n",
    "  # draw the true value line\n",
    "  line_true_val = ax.axhline(y = true_value, color = 'b', linestyle = ':', label = \"true value\")\n",
    "\n",
    "  # draw the averaged incremental reward values for averaging\n",
    "  plt_incr_avgs, = ax.plot(mean_incr_avgs_by_actions, label = \"incremental average\")\n",
    "  # draw the error bar/area for averaging\n",
    "  incr_avgs_minus_std_err = mean_incr_avgs_by_actions - std_err_incr_avgs_by_actions\n",
    "  incr_avgs_plus_std_err = mean_incr_avgs_by_actions + std_err_incr_avgs_by_actions\n",
    "  ax.fill_between(range(0,100), incr_avgs_minus_std_err, incr_avgs_plus_std_err, alpha=0.3)\n",
    "\n",
    "  # draw the averaged incremental reward values for alpha = 0.01\n",
    "  plt_alpha_1_percent, = ax.plot(mean_alpha_1_percent_by_actions, label = \"alpha = 0.01\")\n",
    "  # draw the error bar/area for alpha = 0.01\n",
    "  alpha_1_percent_minus_std_err = mean_alpha_1_percent_by_actions - std_err_alpha_1_percent_by_actions\n",
    "  alpha_1_percent_plus_std_err = mean_alpha_1_percent_by_actions + std_err_alpha_1_percent_by_actions\n",
    "  ax.fill_between(range(0,100), alpha_1_percent_minus_std_err, alpha_1_percent_plus_std_err, alpha=0.3)\n",
    "\n",
    "  # draw the averaged incremental reward values for alpha = 0.1\n",
    "  plt_alpha_10_percent, = ax.plot(mean_alpha_10_percent_by_actions, label = \"alpha = 0.1\")\n",
    "  # draw the error bar/area for alpha = 0.1\n",
    "  alpha_10_percent_minus_std_err = mean_alpha_10_percent_by_actions - std_err_alpha_10_percent_by_actions\n",
    "  alpha_10_percent_plus_std_err = mean_alpha_10_percent_by_actions + std_err_alpha_10_percent_by_actions\n",
    "  ax.fill_between(range(0,100), alpha_10_percent_minus_std_err, alpha_10_percent_plus_std_err, alpha=0.3)\n",
    "\n",
    "  plt_alpha_decay, = ax.plot(mean_alpha_decay_by_actions, label = \"alpha decay\")\n",
    "  alpha_decay_minus_std_err = mean_alpha_decay_by_actions - std_err_alpha_decay_by_actions\n",
    "  alpha_decay_plus_std_err = mean_alpha_decay_by_actions + std_err_alpha_decay_by_actions\n",
    "  ax.fill_between(range(0,100), alpha_decay_minus_std_err, alpha_decay_plus_std_err, alpha=0.3)\n",
    "\n",
    "  ax.set_xlabel(\"sample number\")\n",
    "  ax.set_ylabel(\"reward value\")\n",
    "  ax.set_title(\"Incremental estimates and true expected reward values averaged over 100 runs for action %s\" %action, y=-0.2)\n",
    "\n",
    "  ax.legend(handles=[line_true_val, plt_incr_avgs, plt_alpha_1_percent, plt_alpha_10_percent, plt_alpha_decay])"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HKeI_j9cpvs"
   },
   "source": [
    "### Answers\n",
    "We can see that incremental average starts out the best, however alpha decay quickly catches up to it. Alpha 0.1 also learns relatively quickly, however, we can see that alpha = 0.01 is too slow of a learning rate to catch up with others over 100 trials. \n",
    "\n",
    "It is beneficial to learn a decaying learning rate when we know our sample distributions are stable.\n",
    "\n",
    "If I were to experiment with alpha values, I would try anything between 0.5 to 0.1. Although I would expect 0.5 to be too big of a value which would cause unstability over the prediction since it is not decaying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VL_sU0R5L2se"
   },
   "source": [
    "## Q4 Epsilon-greedy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hEhRJLpKdhK0",
    "ExecuteTime": {
     "end_time": "2025-01-30T19:25:08.770345Z",
     "start_time": "2025-01-30T19:25:08.713192Z"
    }
   },
   "source": [
    "def epsilon_greedy(bandit, epsilon, alpha = None, num_time_step = 1000, epsilon_decay=False, lambda_=0.001, stationary=True, change_time_step=500, bandit_new_means=[]):\n",
    "  \"\"\"Epsilon greedy algorithm for bandit action selection\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  bandit : bandit class\n",
    "      A bernoulli bandit attributes num_arms, and method sample\n",
    "  epsilon: float\n",
    "      A parameter which determines the probability for a random action to be selected\n",
    "  alpha: (optional) float\n",
    "      A parameter which determined the learning rate for averaging. If alpha is none, incremental averaging is used.\n",
    "      Default is none, corresponding to incremental averaging.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  R_over_t\n",
    "      a list of instantaneous return over the time steps\n",
    "  total_R_over_t\n",
    "      a list of cummulative reward over the time steps\n",
    "  est_is_best_over_t\n",
    "      a list of values of 0 and 1 where 1 indicates the estimated best action is the true best action and 0 otherwise for each time step\n",
    "  l_over_t\n",
    "      a list of instanteneous regret over the time steps\n",
    "  total_l_over_t\n",
    "      a list of cummulative regret over the time steps\n",
    "  \"\"\"\n",
    "  # TODO:\n",
    "\n",
    "  num_arms = bandit.num_arms\n",
    "\n",
    "\n",
    "  \n",
    "  Q_arr = np.zeros(num_arms)  # np.zeros(num_arms)  # Estimated Q-values for each arm\n",
    "  N_arr = np.zeros(num_arms)  # Count of times each arm is chosen\n",
    "  total_R = 0 # Total reward\n",
    "  total_l = 0 # total regret\n",
    "  actions = range(num_arms)\n",
    "  # print(bandit.mean, bandit.num_arms)\n",
    "\n",
    "  opt_value =  np.max(bandit.mean) # optimal value, i.e. the mean reward of the best action\n",
    "  # print(\"opt value\", opt_value)  \n",
    "  best_action = np.argmax(bandit.mean) # the arm with the highest mean\n",
    "  # print(\"best action\", best_action) \n",
    "\n",
    "  R_over_t = [] # a list of instantaneous reward over the time steps\n",
    "  total_R_over_t = [] # a list of cummulative reward over the time steps\n",
    "  ith_reward_for_jth_arm = [[] for _ in range(num_arms)] # after a 1000 samples [[rewards when chosen arm0],.., [rewards when chosen arm2]\n",
    "  est_is_best_over_t = [] # a list of values of 0 and 1 where 1 indicates the estimated best action is the true best action and 0 otherwise for each time step\n",
    "  l_over_t = [] # a list of instanteneous regret over the time steps\n",
    "  total_l_over_t = [] # a list of cummulative regret over the time steps\n",
    "  \n",
    "  \n",
    "  epsilon_t = epsilon \n",
    "\n",
    "  for time_step in range(num_time_step):\n",
    "    if not stationary and time_step == change_time_step:\n",
    "        bandit.mean = bandit_new_means\n",
    "        opt_value =  np.max(bandit.mean)\n",
    "        best_action = np.argmax(bandit.mean)\n",
    "        \n",
    "    if epsilon_decay:\n",
    "        epsilon_t = epsilon/(1+lambda_*time_step) \n",
    "     \n",
    "    # A_star is the estimated best action, i.e. exploit (is it estimated or actual best action)\n",
    "    A_star = np.argmax(Q_arr)\n",
    "    A_random = np.random.choice(actions) # Choose a random action, i.e. explore\n",
    "    # print(Q_arr, A_star, A_random)\n",
    "    A = A_random if np.random.uniform() < epsilon_t else A_star  # Chosen action? choose A_star with 1-epsilon probabilty\n",
    "    if 0 in Q_arr: # Will sample randomly until all of the actions are sampled\n",
    "      A = A_random\n",
    "    curr_R = bandit.sample(A)\n",
    "    N_arr[A] += 1 # increase the number of times that the action A was chosen\n",
    "\n",
    "    R_over_t.append(curr_R)\n",
    "    ith_reward_for_jth_arm[A].append(curr_R)\n",
    "\n",
    "    # Commendted out for Question 9\n",
    "    # Update Q-values using incremental averaging or constant step-size\n",
    "    if stationary:\n",
    "      if alpha is None:\n",
    "        Q_arr[A] += (curr_R - Q_arr[A]) / N_arr[A]  # Incremental averaging\n",
    "      else:\n",
    "        Q_arr[A] += alpha * (curr_R - Q_arr[A])  # Constant step-size update\n",
    "    else: # additional code for question 9\n",
    "      alpha = 0.1\n",
    "      Q_arr[A] += alpha * (curr_R - Q_arr[A])\n",
    "\n",
    "    total_R += curr_R\n",
    "    total_R_over_t.append(total_R)\n",
    "  \n",
    "    est_is_best = 1 if A == best_action else 0 # will give 1 if selected action (A) is the best action\n",
    "    est_is_best_over_t.append(est_is_best)\n",
    "  \n",
    "    l_t = opt_value - Q_arr[A]  # Instantenous Regret = Best mean - Expected Value of the action\n",
    "    l_over_t.append(l_t)\n",
    "  \n",
    "    total_l += l_t\n",
    "    total_l_over_t.append(total_l)\n",
    "    # print(np.min(l_over_t))\n",
    "    # print(Q_arr)\n",
    "  return R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t\n",
    "\n",
    "R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = epsilon_greedy(three_arm_gaussian_bandit, 0.01)"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XU1_pP7INeBH"
   },
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JgvBlfQDBJdv",
    "outputId": "ab9398f1-bbd5-4cbd-ffa6-c56f775a1cef",
    "ExecuteTime": {
     "end_time": "2025-01-30T19:25:18.260458Z",
     "start_time": "2025-01-30T19:25:09.618634Z"
    }
   },
   "source": [
    "#TODO:\n",
    "epsilons = [0,1/8,1/4,1/2,1]\n",
    "decaying_epsilon_params = {'epsilon_0':1/2 , 'lambda_':0.1 }  # Decaying epsilon parameters\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(18, 18))\n",
    "\n",
    "for epsilon in epsilons + [\"decay\"]:\n",
    "\n",
    "  # arrays of the data generated from 100 runs\n",
    "  R_over_t_runs = []\n",
    "  total_R_over_t_runs = []\n",
    "  est_is_best_over_t_runs = []\n",
    "  l_over_t_runs = []\n",
    "  total_l_over_t_runs = []\n",
    "\n",
    "  for run in range(100):\n",
    "    if epsilon == \"decay\":\n",
    "      R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = epsilon_greedy(\n",
    "          three_arm_gaussian_bandit, \n",
    "          decaying_epsilon_params['epsilon_0'], \n",
    "          epsilon_decay=True, \n",
    "          lambda_=decaying_epsilon_params['lambda_']\n",
    "      )\n",
    "    else:\n",
    "      R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = epsilon_greedy(\n",
    "          three_arm_gaussian_bandit, \n",
    "          epsilon\n",
    "      )\n",
    "    R_over_t_runs.append(R_over_t)\n",
    "    total_R_over_t_runs.append(total_R_over_t)\n",
    "    est_is_best_over_t_runs.append(est_is_best_over_t)\n",
    "    l_over_t_runs.append(l_over_t)\n",
    "    total_l_over_t_runs.append(total_l_over_t)\n",
    "\n",
    "  R_over_t_runs = np.asarray(R_over_t_runs)\n",
    "  total_R_over_t_runs = np.asarray(total_R_over_t_runs)\n",
    "  est_is_best_over_t_runs = np.asarray(est_is_best_over_t_runs)\n",
    "  l_over_t_runs = np.asarray(l_over_t_runs)\n",
    "  total_l_over_t_runs = np.asarray(total_l_over_t_runs)\n",
    "\n",
    "  # plot the mean reward over time\n",
    "\n",
    "  mean_R_over_t_runs = np.mean(R_over_t_runs, axis=0)\n",
    "  std_err_R_over_t_runs = np.std(R_over_t_runs, axis=0) / np.sqrt(np.size(R_over_t_runs, axis=0))\n",
    "\n",
    "  axs[0,0].plot(mean_R_over_t_runs, label = r\"$\\epsilon = %s$\" %epsilon)\n",
    "\n",
    "  R_over_t_minus_std_err = mean_R_over_t_runs - std_err_R_over_t_runs\n",
    "  R_over_t_plus_std_err = mean_R_over_t_runs  + std_err_R_over_t_runs\n",
    "  axs[0,0].fill_between(range(0,1000), R_over_t_minus_std_err, R_over_t_plus_std_err, alpha=0.4)\n",
    "  # axs[0,0].errorbar(range(0,1000), mean_R_over_t_runs, yerr=std_err_R_over_t_runs)\n",
    "\n",
    "  axs[0,0].legend()\n",
    "  axs[0,0].set_xlabel(\"time step\")\n",
    "  axs[0,0].set_ylabel(\"reward value\")\n",
    "  axs[0,0].set_title(\"Average Instanteneous Reward Received over Time\", y=-0.18)\n",
    "\n",
    "  # plot the mean cummulative reward over time\n",
    "\n",
    "  mean_total_R_over_t_runs = np.mean(total_R_over_t_runs, axis=0)\n",
    "  std_err_total_R_over_t_runs = np.std(total_R_over_t_runs, axis=0) / np.sqrt(np.size(total_R_over_t_runs, axis=0))\n",
    "\n",
    "  axs[0,1].plot(mean_total_R_over_t_runs, label = r\"$\\epsilon = %s$\" %epsilon)\n",
    "\n",
    "  total_R_over_t_minus_std_err = mean_total_R_over_t_runs - std_err_total_R_over_t_runs\n",
    "  total_R_over_t_plus_std_err = mean_total_R_over_t_runs  + std_err_total_R_over_t_runs\n",
    "  axs[0,1].fill_between(range(0,1000), total_R_over_t_minus_std_err, total_R_over_t_plus_std_err, alpha=0.4)\n",
    "\n",
    "  axs[0,1].legend()\n",
    "  axs[0,1].set_xlabel(\"time step\")\n",
    "  axs[0,1].set_ylabel(\"reward value\")\n",
    "  axs[0,1].set_title(\"Average Cummulative Reward Received over Time\", y=-0.18)\n",
    "\n",
    "  #plot the mean percentage of the estimated best action being the first action\n",
    "\n",
    "  est_is_best_over_t_runs_avgs = np.mean(est_is_best_over_t_runs, axis=0)\n",
    "  plt_est_is_best_over_t_runs_avgs, = axs[1,0].plot(est_is_best_over_t_runs_avgs, label = r\"$\\epsilon = %s$\" %epsilon)\n",
    "\n",
    "  axs[1,0].legend()\n",
    "  axs[1,0].set_xlabel(\"time step\")\n",
    "  axs[1,0].set_ylabel(\"percentage\")\n",
    "  axs[1,0].set_title(\"Percentage of the Estimated Best Action Being the First Action\", y=-0.18)\n",
    "\n",
    "  #plot the mean instantaneous regret over time\n",
    "\n",
    "  l_over_t_runs_avgs = np.mean(l_over_t_runs, axis=0)\n",
    "  axs[1,1].plot(l_over_t_runs_avgs, label = r\"$\\epsilon = %s$\" %epsilon)\n",
    "\n",
    "  axs[1,1].legend()\n",
    "  axs[1,1].set_xlabel(\"time step\")\n",
    "  axs[1,1].set_ylabel(\"regret\")\n",
    "  axs[1,1].set_title(\"Instantaneous Regret over Time\", y=-0.18)\n",
    "\n",
    "  #plot the total regret over time\n",
    "\n",
    "  total_l_over_t_runs_avgs = np.mean(total_l_over_t_runs, axis=0)\n",
    "  axs[2,0].plot(total_l_over_t_runs_avgs, label = r\"$\\epsilon = %s$\" %epsilon)\n",
    "\n",
    "  axs[2,0].legend()\n",
    "  axs[2,0].set_xlabel(\"time step\")\n",
    "  axs[2,0].set_ylabel(\"regret\")\n",
    "  axs[2,0].set_title(\"Total Regret up to Time Step t\", y=-0.18)\n",
    "\n",
    "axs[-1, -1].axis('off')\n",
    "\n",
    "title = r'Graphs  for Epsilon Greedy with Varying Epsilons'\n",
    "fig.suptitle(title, fontsize=16, y=0.08)\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILoBgOrocu_z"
   },
   "source": [
    "### Answers\n",
    "Since I intialized the model in a way that it will randomly sample actions unless all actions have non-zero Q value prediction,and the standard deviation for rewards is not big enough to confuse the \"model\", when we have epsilon 0, it always predicts to most optimal arm, resulting in near-0 regret (Epsilon-decay also learns very fast and on par with epsilon = 0). When I don't initialize the model in this way, as in, there is no condition for every arm to be sampled at least once then epsilon = 0 and epsilon = 1 act the same over 100 of trials. Since epsilon 0 sticks with the first random choice makes.\n",
    "Besides that we see that as Epsilon increases the total regret increases and total reward decreases. We can attribute this to our arms being stationary with relatively small standard deviations and after a few samples, we could have stopped exploring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qULufDuyXgb"
   },
   "source": [
    "## Q5 Hyperparameters for Epsilon-greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVbJAAc0Ebc6"
   },
   "source": [
    "To have a plain start, you have been provided with predefined functions for generating plots until now. However, moving forward, you are expected to plot graphs on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jH8EgaKmvEbz"
   },
   "source": [
    "### Graphs"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T19:25:39.886117Z",
     "start_time": "2025-01-30T19:25:25.379305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epsilons = [1/8, 1/4]\n",
    "decaying_epsilon_params = {'epsilon_0': 1/2, 'lambda_': 0.1}\n",
    "alphas = [0.1, 0.01, 0.001]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(18, 18))\n",
    "\n",
    "for epsilon in epsilons + [\"decay\"]:  # Iterate over epsilon values\n",
    "  for a in alphas:  # Iterate over alpha values\n",
    "\n",
    "    # Arrays of data from 100 runs\n",
    "    R_over_t_runs, total_R_over_t_runs = [], []\n",
    "    est_is_best_over_t_runs, l_over_t_runs, total_l_over_t_runs = [], [], []\n",
    "\n",
    "    for run in range(100):\n",
    "      if epsilon == \"decay\":\n",
    "        R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = epsilon_greedy(\n",
    "          three_arm_gaussian_bandit,\n",
    "          decaying_epsilon_params['epsilon_0'],\n",
    "          epsilon_decay=True,\n",
    "          lambda_=decaying_epsilon_params['lambda_'],\n",
    "          alpha=a\n",
    "        )\n",
    "      else:\n",
    "        R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = epsilon_greedy(\n",
    "          three_arm_gaussian_bandit,\n",
    "          epsilon,\n",
    "          alpha=a\n",
    "        )\n",
    "\n",
    "      # Store results\n",
    "      R_over_t_runs.append(R_over_t)\n",
    "      total_R_over_t_runs.append(total_R_over_t)\n",
    "      est_is_best_over_t_runs.append(est_is_best_over_t)\n",
    "      l_over_t_runs.append(l_over_t)\n",
    "      total_l_over_t_runs.append(total_l_over_t)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    R_over_t_runs = np.array(R_over_t_runs)\n",
    "    total_R_over_t_runs = np.array(total_R_over_t_runs)\n",
    "    est_is_best_over_t_runs = np.array(est_is_best_over_t_runs)\n",
    "    l_over_t_runs = np.array(l_over_t_runs)\n",
    "    total_l_over_t_runs = np.array(total_l_over_t_runs)\n",
    "\n",
    "    # Check if arrays have valid data\n",
    "    if R_over_t_runs.size == 0 or total_R_over_t_runs.size == 0:\n",
    "      print(f\"Warning: Empty data for epsilon={epsilon}, alpha={a}\")\n",
    "      continue  # Skip plotting if data is missing\n",
    "\n",
    "    # Compute means and standard errors\n",
    "    mean_R_over_t_runs = np.mean(R_over_t_runs, axis=0)\n",
    "    std_err_R_over_t_runs = np.std(R_over_t_runs, axis=0) / np.sqrt(R_over_t_runs.shape[0])\n",
    "\n",
    "    mean_total_R_over_t_runs = np.mean(total_R_over_t_runs, axis=0)\n",
    "    std_err_total_R_over_t_runs = np.std(total_R_over_t_runs, axis=0) / np.sqrt(total_R_over_t_runs.shape[0])\n",
    "\n",
    "    # Plot: Instantaneous Reward\n",
    "    axs[0, 0].plot(mean_R_over_t_runs, label=f\"$\\\\epsilon = {epsilon}, \\\\alpha = {a}$\")\n",
    "    axs[0, 0].fill_between(range(0, 1000), mean_R_over_t_runs - std_err_R_over_t_runs,\n",
    "                           mean_R_over_t_runs + std_err_R_over_t_runs, alpha=0.3)\n",
    "\n",
    "    # Plot: Cumulative Reward\n",
    "    axs[0, 1].plot(mean_total_R_over_t_runs, label=f\"$\\\\epsilon = {epsilon}, \\\\alpha = {a}$\")\n",
    "    axs[0, 1].fill_between(range(0, 1000), mean_total_R_over_t_runs - std_err_total_R_over_t_runs,\n",
    "                           mean_total_R_over_t_runs + std_err_total_R_over_t_runs, alpha=0.3)\n",
    "\n",
    "    # Plot: Estimated Best Action\n",
    "    axs[1, 0].plot(np.mean(est_is_best_over_t_runs, axis=0), label=f\"$\\\\epsilon = {epsilon}, \\\\alpha = {a}$\")\n",
    "\n",
    "    # Plot: Instantaneous Regret\n",
    "    axs[1, 1].plot(np.mean(l_over_t_runs, axis=0), label=f\"$\\\\epsilon = {epsilon}, \\\\alpha = {a}$\")\n",
    "\n",
    "    # Plot: Total Regret\n",
    "    axs[2, 0].plot(np.mean(total_l_over_t_runs, axis=0), label=f\"$\\\\epsilon = {epsilon}, \\\\alpha = {a}$\")\n",
    "\n",
    "# Adjust subplot settings\n",
    "for ax in axs.flat:\n",
    "  if len(ax.get_lines()) > 0:  # Only show legend if there are plotted lines\n",
    "    ax.legend()\n",
    "  else:\n",
    "    ax.legend([], [], loc='upper right', handlelength=0)  # Prevents warning\n",
    "\n",
    "  ax.set_xlabel(\"Time Step\")\n",
    "\n",
    "axs[0, 0].set_ylabel(\"Reward Value\")\n",
    "axs[0, 0].set_title(\"Average Instantaneous Reward over Time\")\n",
    "\n",
    "axs[0, 1].set_ylabel(\"Reward Value\")\n",
    "axs[0, 1].set_title(\"Average Cumulative Reward over Time\")\n",
    "\n",
    "axs[1, 0].set_ylabel(\"Percentage\")\n",
    "axs[1, 0].set_title(\"Estimated Best Action Selection Percentage\")\n",
    "\n",
    "axs[1, 1].set_ylabel(\"Regret\")\n",
    "axs[1, 1].set_title(\"Instantaneous Regret over Time\")\n",
    "\n",
    "axs[2, 0].set_ylabel(\"Regret\")\n",
    "axs[2, 0].set_title(\"Total Regret up to Time Step t\")\n",
    "\n",
    "# Hide last unused subplot\n",
    "axs[2, 1].axis('off')\n",
    "\n",
    "# Adjust layout\n",
    "fig.suptitle(\"Epsilon-Greedy Performance Analysis\", fontsize=16, y=0.92)\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "plt.show()\n"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRVYgAJPczbo"
   },
   "source": [
    "### Answers\n",
    "All epsilon values perform better (minimize regret, maximize reward) when they are paired with a higher alpha learning rate, best being alpha = 0.1. Epsilon 0.125 (smallest epsilon) and alpha 0.1 (biggest alpha) gives the best results overall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzcTHHnbEZbZ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5ttd7oJXiQe"
   },
   "source": [
    "## Q6 Gradient Bandit\n",
    "[20 points] Write a function that implements the gradient bandit algorithm discussed in class. The\n",
    " algorithm should use a baseline to normalize the rewards and update preferences for actions based\n",
    " on the gradient ascent formula. Plot the same graphs as above for α = 0.1, α = 0.01, α = 0.001\n",
    " and a decaying learning rate defined as αt = α0/\n",
    " (1+λt)**p \n",
    ", with α0 = 0.5, λ = 0.01, and p = 0.5.\n",
    " Explain briefly the behavior you observe."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Wed5NdLZXjrE",
    "ExecuteTime": {
     "end_time": "2025-01-30T19:25:39.893902Z",
     "start_time": "2025-01-30T19:25:39.886117Z"
    }
   },
   "source": [
    "def gradient_bandit(bandit, alpha = 0.1, alpha_decay = False, num_time_step = 1000, lambda_=0.01, p=0.5, stationary=True, change_time_step=500, bandit_new_means=[]):\n",
    "  H_t = [0.0 for _ in range(bandit.num_arms)]  # H_t is the preference for each action at time step t, initialized to be 0 for all action at t = 0\n",
    "  R_avg = 0.0 # Average Reward\n",
    "  R_total = 0.0 # Total Reward\n",
    "  Regret_avg = 0.0 \n",
    "  Regret_total = 0.0\n",
    "  \n",
    "  \n",
    "  alpha_0 = alpha\n",
    "\n",
    "  opt_value =  np.max(bandit.mean) # optimal value, i.e. the mean reward of the best action\n",
    "  # print(\"opt value\", opt_value)  \n",
    "  best_action = np.argmax(bandit.mean) # the arm with the highest mean\n",
    "  # print(\"best action\", best_action) \n",
    "  \n",
    "  R_over_t = [] # a list of instantaneous reward over the time steps\n",
    "  total_R_over_t = [] # a list of cummulative reward over the time steps\n",
    "  est_is_best_over_t = [] # a list of values of 0 and 1 where 1 indicates the estimated best action is the true best action and 0 otherwise for each time step\n",
    "  l_over_t = [] # a list of instanteneous regret over the time steps\n",
    "  total_l_over_t = [] # a list of cummulative regret over the time steps\n",
    "\n",
    "  \n",
    "  for t in range(1, num_time_step + 1):\n",
    "    if not stationary and t == change_time_step:\n",
    "      bandit.mean = bandit_new_means\n",
    "      opt_value =  np.max(bandit.mean)\n",
    "      best_action = np.argmax(bandit.mean)\n",
    "    # look through the preferences\n",
    "    # get the argmax for the chosen action \n",
    "    # if 0.0 in H_t, then chose action randomly\n",
    "\n",
    "    # Compute action probabilities using standard softmax\n",
    "    exp_H = [math.exp(h) for h in H_t]\n",
    "    sum_exp_H = sum(exp_H)\n",
    "    pi = [e_h / sum_exp_H for e_h in exp_H]\n",
    "\n",
    "    # Select action A based on probabilities pi\n",
    "    A = random.choices(range(bandit.num_arms), weights=pi, k=1)[0] # random choices returns a list with the index of the selected probability, e.g. [0] or [2]\n",
    "\n",
    "    R = bandit.sample(A)\n",
    "    R_over_t.append(R)\n",
    "    R_total += R\n",
    "    total_R_over_t.append(R_total)\n",
    "    # R_avg += (R - R_avg)/t commented for ninth question\n",
    "    # 2 lines of code for question 9\n",
    "    update_factor = (1 / t) if stationary else alpha\n",
    "    R_avg += update_factor * (R - R_avg)\n",
    "\n",
    "    Regret = opt_value - R # since we dont know the Q value we use the actual reward to calculate regret\n",
    "    l_over_t.append(Regret)\n",
    "    Regret_total += Regret\n",
    "    total_l_over_t.append(Regret_total)\n",
    "\n",
    "\n",
    "    est_is_best = 1 if A == best_action else 0 # will give 1 if selected action (A) is the best action\n",
    "    est_is_best_over_t.append(est_is_best)\n",
    "    \n",
    "    # If decaying alpha is used update alpha\n",
    "    if alpha_decay:\n",
    "      alpha_t = alpha_0/(1+lambda_*t)**p\n",
    "    else:\n",
    "      alpha_t = alpha\n",
    "\n",
    "    # Update preferences using gradient ascent\n",
    "    for a in range(bandit.num_arms):\n",
    "      if a == A:\n",
    "        H_t[a] += alpha_t * (R - R_avg) * (1 - pi[a])\n",
    "      else:\n",
    "        H_t[a] += alpha_t * (R - R_avg) * (0 - pi[a])\n",
    "\n",
    "  return R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqUx-AYVvu2B"
   },
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "P9ueHpp7Z7e1",
    "outputId": "44651c79-3f0c-4c9b-b173-3f87b6635a3c",
    "ExecuteTime": {
     "end_time": "2025-01-30T19:25:47.213940Z",
     "start_time": "2025-01-30T19:25:43.988735Z"
    }
   },
   "source": [
    "#TODO:\n",
    "alphas = [0.1,0.01,0.001]\n",
    "# decaying_alpha_params = {'alpha_0': 0.5, 'lambda_': 0.1}  # Decaying alpha parameters\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(18, 18))\n",
    "\n",
    "for alpha in alphas + [\"decay\"]:\n",
    "\n",
    "  # arrays of the data generated from 100 runs\n",
    "  R_over_t_runs = []\n",
    "  total_R_over_t_runs = []\n",
    "  est_is_best_over_t_runs = []\n",
    "  l_over_t_runs = []\n",
    "  total_l_over_t_runs = []\n",
    "\n",
    "  for run in range(100):\n",
    "    if alpha == \"decay\":\n",
    "      R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = gradient_bandit(three_arm_gaussian_bandit, alpha = 0.5, alpha_decay = True, num_time_step = 1000, lambda_=0.1, p=0.5, stationary = True)\n",
    "    else:\n",
    "      R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = gradient_bandit(three_arm_gaussian_bandit, alpha = alpha, alpha_decay = False, num_time_step = 1000, lambda_=0.1, p=0.5, stationary = True)\n",
    "        \n",
    "    R_over_t_runs.append(R_over_t)\n",
    "    total_R_over_t_runs.append(total_R_over_t)\n",
    "    est_is_best_over_t_runs.append(est_is_best_over_t)\n",
    "    l_over_t_runs.append(l_over_t)\n",
    "    total_l_over_t_runs.append(total_l_over_t)\n",
    "\n",
    "  R_over_t_runs = np.asarray(R_over_t_runs)\n",
    "  total_R_over_t_runs = np.asarray(total_R_over_t_runs)\n",
    "  est_is_best_over_t_runs = np.asarray(est_is_best_over_t_runs)\n",
    "  l_over_t_runs = np.asarray(l_over_t_runs)\n",
    "  total_l_over_t_runs = np.asarray(total_l_over_t_runs)\n",
    "\n",
    "  # plot the mean reward over time\n",
    "\n",
    "  mean_R_over_t_runs = np.mean(R_over_t_runs, axis=0)\n",
    "  # print(mean_R_over_t_runs)\n",
    "  std_err_R_over_t_runs = np.std(R_over_t_runs, axis=0) / np.sqrt(np.size(R_over_t_runs, axis=0))\n",
    "\n",
    "  axs[0,0].plot(mean_R_over_t_runs, label = r\"$\\alpha = %s$\" %alpha)\n",
    "\n",
    "  R_over_t_minus_std_err = mean_R_over_t_runs - std_err_R_over_t_runs\n",
    "  R_over_t_plus_std_err = mean_R_over_t_runs  + std_err_R_over_t_runs\n",
    "  axs[0,0].fill_between(range(0,1000), R_over_t_minus_std_err, R_over_t_plus_std_err, alpha=0.4)\n",
    "  # axs[0,0].errorbar(range(0,1000), mean_R_over_t_runs, yerr=std_err_R_over_t_runs)\n",
    "\n",
    "  axs[0,0].legend()\n",
    "  axs[0,0].set_xlabel(\"time step\")\n",
    "  axs[0,0].set_ylabel(\"reward value\")\n",
    "  axs[0,0].set_title(\"Average Instantaneous Reward Received over Time\", y=-0.18)\n",
    "\n",
    "  # plot the mean cummulative reward over time\n",
    "\n",
    "  mean_total_R_over_t_runs = np.mean(total_R_over_t_runs, axis=0) # different alphas seem to be performing exactly the same\n",
    "  std_err_total_R_over_t_runs = np.std(total_R_over_t_runs, axis=0) / np.sqrt(np.size(total_R_over_t_runs, axis=0))\n",
    "\n",
    "  axs[0,1].plot(mean_total_R_over_t_runs, label = r\"$\\alpha = %s$\" %alpha)\n",
    "\n",
    "  total_R_over_t_minus_std_err = mean_total_R_over_t_runs - std_err_total_R_over_t_runs\n",
    "  total_R_over_t_plus_std_err = mean_total_R_over_t_runs  + std_err_total_R_over_t_runs\n",
    "  axs[0,1].fill_between(range(0,1000), total_R_over_t_minus_std_err, total_R_over_t_plus_std_err, alpha=0.4)\n",
    "\n",
    "  axs[0,1].legend()\n",
    "  axs[0,1].set_xlabel(\"time step\")\n",
    "  axs[0,1].set_ylabel(\"reward value\")\n",
    "  axs[0,1].set_title(\"Average Cumulative Reward Received over Time\", y=-0.18)\n",
    "\n",
    "  #plot the mean percentage of the estimated best action being the third action\n",
    "\n",
    "  est_is_best_over_t_runs_avgs = np.mean(est_is_best_over_t_runs, axis=0)\n",
    "  plt_est_is_best_over_t_runs_avgs, = axs[1,0].plot(est_is_best_over_t_runs_avgs, label = r\"$\\alpha = %s$\" %alpha)\n",
    "\n",
    "  axs[1,0].legend()\n",
    "  axs[1,0].set_xlabel(\"time step\")\n",
    "  axs[1,0].set_ylabel(\"percentage\")\n",
    "  axs[1,0].set_title(\"Percentage of Runs where Best Action was Chosen\", y=-0.18)\n",
    "\n",
    "  #plot the mean instantaneous regret over time\n",
    "\n",
    "  l_over_t_runs_avgs = np.mean(l_over_t_runs, axis=0)\n",
    "  axs[1,1].plot(l_over_t_runs_avgs, label = r\"$\\alpha = %s$\" %alpha)\n",
    "\n",
    "  axs[1,1].legend()\n",
    "  axs[1,1].set_xlabel(\"time step\")\n",
    "  axs[1,1].set_ylabel(\"regret\")\n",
    "  axs[1,1].set_title(\"Instantaneous Regret over Time\", y=-0.18)\n",
    "\n",
    "  #plot the total regret over time\n",
    "\n",
    "  total_l_over_t_runs_avgs = np.mean(total_l_over_t_runs, axis=0)\n",
    "  axs[2,0].plot(total_l_over_t_runs_avgs, label = r\"$\\alpha = %s$\" %alpha)\n",
    "\n",
    "  axs[2,0].legend()\n",
    "  axs[2,0].set_xlabel(\"time step\")\n",
    "  axs[2,0].set_ylabel(\"regret\")\n",
    "  axs[2,0].set_title(\"Total Regret up to Time Step t\", y=-0.18)\n",
    "\n",
    "axs[-1, -1].axis('off')\n",
    "\n",
    "title = r'Graphs for Gradient bandit with different alphas and alpha decay'\n",
    "fig.suptitle(title, fontsize=16, y=0.08)\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NABR2XVSc2fk"
   },
   "source": [
    "### Answers\n",
    "Instantaneous regret gets close to 0 over time faster with higher alpha values or alpha decay. Small alpha values have a hard time \"learning\" thus the instantaneous regret doesn't really go down over 1000 samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xw4DpXavIoue"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9GklWhZM9um"
   },
   "source": [
    "## Q7 Thompson Sampling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CAlN8q-YM_Mt",
    "ExecuteTime": {
     "end_time": "2025-01-30T19:26:33.609495Z",
     "start_time": "2025-01-30T19:26:33.599032Z"
    }
   },
   "source": [
    "def thompson_sampling_gaussian(bandit, num_time_step=1000, stationary=True, change_time_step=500, bandit_new_means=[]):\n",
    "\n",
    "  num_arms = bandit.num_arms\n",
    "  variance = bandit.variance\n",
    "\n",
    "  # Initialize posterior parameters for each arm\n",
    "  # Assume initial prior mean=0 and precision=1 (variance=1)\n",
    "  posterior_means = [0.0 for _ in range(num_arms)]\n",
    "  posterior_precisions = [1.0 for _ in range(num_arms)]  # Precision = 1 / variance\n",
    "\n",
    "  R_total = 0.0  # Total Reward\n",
    "  Regret_total = 0.0  # Total Regret\n",
    "\n",
    "  opt_value = np.max(bandit.mean)  # Optimal mean reward\n",
    "  best_action = np.argmax(bandit.mean)  # Best arm\n",
    "\n",
    "  R_over_t = []\n",
    "  total_R_over_t = []\n",
    "  est_is_best_over_t = []\n",
    "  l_over_t = []\n",
    "  total_l_over_t = []\n",
    "\n",
    "  for t in range(1, num_time_step + 1):\n",
    "    if not stationary and t == change_time_step:\n",
    "      bandit.mean = bandit_new_means\n",
    "      opt_value =  np.max(bandit.mean)\n",
    "      best_action = np.argmax(bandit.mean)\n",
    "    # Sample from the posterior for each arm\n",
    "    sampled_means = [\n",
    "      np.random.normal(posterior_means[a], np.sqrt(1 / posterior_precisions[a]))\n",
    "      for a in range(num_arms)\n",
    "    ]\n",
    "\n",
    "    # Select the arm with the highest sampled mean\n",
    "    A = np.argmax(sampled_means)\n",
    "\n",
    "    # Pull the selected arm and observe the reward\n",
    "    R = bandit.sample(A)\n",
    "    R_over_t.append(R)\n",
    "    R_total += R\n",
    "    total_R_over_t.append(R_total)\n",
    "\n",
    "    # Calculate instantaneous regret using actual reward\n",
    "    Regret = opt_value - R\n",
    "    l_over_t.append(Regret)\n",
    "    Regret_total += Regret\n",
    "    total_l_over_t.append(Regret_total)\n",
    "\n",
    "    # Check if the selected action is the best action\n",
    "    est_is_best = 1 if A == best_action else 0\n",
    "    est_is_best_over_t.append(est_is_best)\n",
    "\n",
    "    # Update the posterior for the selected arm\n",
    "    # Assuming known variance, update posterior mean and precision\n",
    "    # Using conjugate priors for Gaussian\n",
    "    # Posterior precision: prior precision + 1/variance\n",
    "    posterior_precisions[A] = (posterior_precisions[A] + 1 / variance) if stationary else (0.9 * posterior_precisions[A] + 1 / variance) # decay factor that adapts to predictions to unstationary problems, for question 9\n",
    "\n",
    "\n",
    "    # Posterior mean: (prior precision * prior mean + observed reward / variance) / posterior precision\n",
    "    posterior_means[A] = (posterior_precisions[A] * posterior_means[A] + R / variance) / posterior_precisions[A]\n",
    "\n",
    "  return R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t\n",
    "\n"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG3KAgC5v2gl"
   },
   "source": [
    "### Graphs"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T19:26:36.905926Z",
     "start_time": "2025-01-30T19:26:34.532757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(18, 18))\n",
    "\n",
    "# arrays of the data generated from 100 runs\n",
    "R_over_t_runs = []\n",
    "total_R_over_t_runs = []\n",
    "est_is_best_over_t_runs = []\n",
    "l_over_t_runs = []\n",
    "total_l_over_t_runs = []\n",
    "\n",
    "for run in range(100):\n",
    "  R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = thompson_sampling_gaussian(three_arm_gaussian_bandit, num_time_step=1000)\n",
    "\n",
    "  R_over_t_runs.append(R_over_t)\n",
    "  total_R_over_t_runs.append(total_R_over_t)\n",
    "  est_is_best_over_t_runs.append(est_is_best_over_t)\n",
    "  l_over_t_runs.append(l_over_t)\n",
    "  total_l_over_t_runs.append(total_l_over_t)\n",
    "\n",
    "R_over_t_runs = np.asarray(R_over_t_runs)\n",
    "total_R_over_t_runs = np.asarray(total_R_over_t_runs)\n",
    "est_is_best_over_t_runs = np.asarray(est_is_best_over_t_runs)\n",
    "l_over_t_runs = np.asarray(l_over_t_runs)\n",
    "total_l_over_t_runs = np.asarray(total_l_over_t_runs)\n",
    "\n",
    "# Plot the mean reward over time\n",
    "mean_R_over_t_runs = np.mean(R_over_t_runs, axis=0)\n",
    "std_err_R_over_t_runs = np.std(R_over_t_runs, axis=0) / np.sqrt(R_over_t_runs.shape[0])\n",
    "\n",
    "axs[0,0].plot(mean_R_over_t_runs, label='Mean Reward')\n",
    "R_over_t_minus_std_err = mean_R_over_t_runs - std_err_R_over_t_runs\n",
    "R_over_t_plus_std_err = mean_R_over_t_runs + std_err_R_over_t_runs\n",
    "axs[0,0].fill_between(range(1000), R_over_t_minus_std_err, R_over_t_plus_std_err, alpha=0.4, label='Std Error')\n",
    "axs[0,0].legend()\n",
    "axs[0,0].set_xlabel(\"Time Step\")\n",
    "axs[0,0].set_ylabel(\"Reward\")\n",
    "axs[0,0].set_title(\"Average Instantaneous Reward Received over Time\", y=-0.18)\n",
    "\n",
    "# Plot the mean cumulative reward over time\n",
    "mean_total_R_over_t_runs = np.mean(total_R_over_t_runs, axis=0)\n",
    "std_err_total_R_over_t_runs = np.std(total_R_over_t_runs, axis=0) / np.sqrt(total_R_over_t_runs.shape[0])\n",
    "\n",
    "axs[0,1].plot(mean_total_R_over_t_runs, label='Mean Cumulative Reward')\n",
    "total_R_over_t_minus_std_err = mean_total_R_over_t_runs - std_err_total_R_over_t_runs\n",
    "total_R_over_t_plus_std_err = mean_total_R_over_t_runs + std_err_total_R_over_t_runs\n",
    "axs[0,1].fill_between(range(1000), total_R_over_t_minus_std_err, total_R_over_t_plus_std_err, alpha=0.4, label='Std Error')\n",
    "axs[0,1].legend()\n",
    "axs[0,1].set_xlabel(\"Time Step\")\n",
    "axs[0,1].set_ylabel(\"Cumulative Reward\")\n",
    "axs[0,1].set_title(\"Average Cumulative Reward Received over Time\", y=-0.18)\n",
    "\n",
    "# Plot the mean percentage of the estimated best action being the best\n",
    "est_is_best_over_t_runs_avgs = np.mean(est_is_best_over_t_runs, axis=0)\n",
    "axs[1,0].plot(est_is_best_over_t_runs_avgs, label='Best Action Percentage')\n",
    "\n",
    "axs[1,0].legend()\n",
    "axs[1,0].set_xlabel(\"Time Step\")\n",
    "axs[1,0].set_ylabel(\"Percentage\")\n",
    "axs[1,0].set_title(\"Percentage of Runs where Best Action was Chosen\", y=-0.18)\n",
    "\n",
    "# Plot the mean instantaneous regret over time\n",
    "l_over_t_runs_avgs = np.mean(l_over_t_runs, axis=0)\n",
    "axs[1,1].plot(l_over_t_runs_avgs, label='Instantaneous Regret')\n",
    "\n",
    "axs[1,1].legend()\n",
    "axs[1,1].set_xlabel(\"Time Step\")\n",
    "axs[1,1].set_ylabel(\"Regret\")\n",
    "axs[1,1].set_title(\"Instantaneous Regret over Time\", y=-0.18)\n",
    "\n",
    "# Plot the total regret over time\n",
    "total_l_over_t_runs_avgs = np.mean(total_l_over_t_runs, axis=0)\n",
    "axs[2,0].plot(total_l_over_t_runs_avgs, label='Total Regret')\n",
    "\n",
    "axs[2,0].legend()\n",
    "axs[2,0].set_xlabel(\"Time Step\")\n",
    "axs[2,0].set_ylabel(\"Regret\")\n",
    "axs[2,0].set_title(\"Total Regret up to Time Step t\", y=-0.18)\n",
    "\n",
    "axs[-1, -1].axis('off')\n",
    "\n",
    "title = r'Graphs for Thompson Sampling'\n",
    "fig.suptitle(title, fontsize=16, y=0.08)\n",
    "\n",
    "plt.show()\n"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRc45fxdc46B"
   },
   "source": [
    "### Answers\n",
    "The algorithm manages to learn which action is the best very quickly and drops its instantenous regret, however, we couldn't get logarithmic total regret which is expected from thomson sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxB0EtT4MLnO"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggb7m7AwcFb1"
   },
   "source": [
    "## Q8 Comparison of Algorithms"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T19:26:36.913087Z",
     "start_time": "2025-01-30T19:26:36.905926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def choose_algorithms(algorithm, bandit, num_time_step=1000, stationary=True, change_time_step=500, bandit_new_means=[]):\n",
    "\n",
    "  if algorithm == \"epsilon\":\n",
    "    # Epsilon-greedy, Epsilon = 0\n",
    "    R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t  = epsilon_greedy(bandit, epsilon=0, num_time_step=num_time_step, stationary=True, change_time_step=500, bandit_new_means=bandit_new_means)\n",
    "\n",
    "  if algorithm == \"gradient\":\n",
    "    # Gradient bandit, alpha decay with alpha_0 = 0.5\n",
    "    R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t  = gradient_bandit(three_arm_gaussian_bandit, alpha = 0.5, alpha_decay = True, num_time_step = 1000, lambda_=0.1, p=0.5,stationary=True, change_time_step=500, bandit_new_means=bandit_new_means)\n",
    "\n",
    "  if algorithm == \"thompson\":\n",
    "    # Thompson sampling\n",
    "    R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = thompson_sampling_gaussian(bandit, num_time_step=num_time_step, stationary=True, change_time_step=500, bandit_new_means=bandit_new_means)\n",
    "\n",
    "  return (R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t)"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMyZZ1P1PNqU"
   },
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RBo6IK0x6wBJ",
    "outputId": "6aa51e57-03e2-4514-baf2-8420aa9b38fc",
    "ExecuteTime": {
     "end_time": "2025-01-30T19:26:42.462223Z",
     "start_time": "2025-01-30T19:26:38.248506Z"
    }
   },
   "source": [
    "algorithm_list = [\"epsilon\", \"gradient\", \"thompson\"]\n",
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(18, 18))\n",
    "\n",
    "for algorithm in algorithm_list:\n",
    "\n",
    "  # arrays of the data generated from 100 runs\n",
    "  R_over_t_runs = []\n",
    "  total_R_over_t_runs = []\n",
    "  est_is_best_over_t_runs = []\n",
    "  l_over_t_runs = []\n",
    "  total_l_over_t_runs = []\n",
    "\n",
    "  for run in range(100):\n",
    "      R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = choose_algorithms(algorithm, three_arm_gaussian_bandit, num_time_step=1000)\n",
    "\n",
    "      R_over_t_runs.append(R_over_t)\n",
    "      total_R_over_t_runs.append(total_R_over_t)\n",
    "      est_is_best_over_t_runs.append(est_is_best_over_t)\n",
    "      l_over_t_runs.append(l_over_t)\n",
    "      total_l_over_t_runs.append(total_l_over_t)\n",
    "\n",
    "  R_over_t_runs = np.asarray(R_over_t_runs)\n",
    "  total_R_over_t_runs = np.asarray(total_R_over_t_runs)\n",
    "  est_is_best_over_t_runs = np.asarray(est_is_best_over_t_runs)\n",
    "  l_over_t_runs = np.asarray(l_over_t_runs)\n",
    "  total_l_over_t_runs = np.asarray(total_l_over_t_runs)\n",
    "\n",
    "  # plot the mean reward over time\n",
    "\n",
    "  mean_R_over_t_runs = np.mean(R_over_t_runs, axis=0)\n",
    "  # print(mean_R_over_t_runs)\n",
    "  std_err_R_over_t_runs = np.std(R_over_t_runs, axis=0) / np.sqrt(np.size(R_over_t_runs, axis=0))\n",
    "\n",
    "  axs[0,0].plot(mean_R_over_t_runs, label = algorithm)\n",
    "\n",
    "  R_over_t_minus_std_err = mean_R_over_t_runs - std_err_R_over_t_runs\n",
    "  R_over_t_plus_std_err = mean_R_over_t_runs  + std_err_R_over_t_runs\n",
    "  axs[0,0].fill_between(range(0,1000), R_over_t_minus_std_err, R_over_t_plus_std_err, alpha=0.4)\n",
    "  # axs[0,0].errorbar(range(0,1000), mean_R_over_t_runs, yerr=std_err_R_over_t_runs)\n",
    "\n",
    "  axs[0,0].legend()\n",
    "  axs[0,0].set_xlabel(\"time step\")\n",
    "  axs[0,0].set_ylabel(\"reward value\")\n",
    "  axs[0,0].set_title(\"Average Instantaneous Reward Received over Time\", y=-0.18)\n",
    "\n",
    "  # plot the mean cummulative reward over time\n",
    "\n",
    "  mean_total_R_over_t_runs = np.mean(total_R_over_t_runs, axis=0) # different alphas seem to be performing exactly the same\n",
    "  std_err_total_R_over_t_runs = np.std(total_R_over_t_runs, axis=0) / np.sqrt(np.size(total_R_over_t_runs, axis=0))\n",
    "\n",
    "  axs[0,1].plot(mean_total_R_over_t_runs, label = algorithm)\n",
    "\n",
    "  total_R_over_t_minus_std_err = mean_total_R_over_t_runs - std_err_total_R_over_t_runs\n",
    "  total_R_over_t_plus_std_err = mean_total_R_over_t_runs  + std_err_total_R_over_t_runs\n",
    "  axs[0,1].fill_between(range(0,1000), total_R_over_t_minus_std_err, total_R_over_t_plus_std_err, alpha=0.4)\n",
    "\n",
    "  axs[0,1].legend()\n",
    "  axs[0,1].set_xlabel(\"time step\")\n",
    "  axs[0,1].set_ylabel(\"reward value\")\n",
    "  axs[0,1].set_title(\"Average Cumulative Reward Received over Time\", y=-0.18)\n",
    "\n",
    "  #plot the mean percentage of the estimated best action being the third action\n",
    "\n",
    "  est_is_best_over_t_runs_avgs = np.mean(est_is_best_over_t_runs, axis=0)\n",
    "  plt_est_is_best_over_t_runs_avgs, = axs[1,0].plot(est_is_best_over_t_runs_avgs,label = algorithm)\n",
    "\n",
    "  axs[1,0].legend()\n",
    "  axs[1,0].set_xlabel(\"time step\")\n",
    "  axs[1,0].set_ylabel(\"percentage\")\n",
    "  axs[1,0].set_title(\"Percentage of Runs where Best Action was Chosen\", y=-0.18)\n",
    "\n",
    "  #plot the mean instantaneous regret over time\n",
    "\n",
    "  l_over_t_runs_avgs = np.mean(l_over_t_runs, axis=0)\n",
    "  axs[1,1].plot(l_over_t_runs_avgs, label = algorithm)\n",
    "\n",
    "  axs[1,1].legend()\n",
    "  axs[1,1].set_xlabel(\"time step\")\n",
    "  axs[1,1].set_ylabel(\"regret\")\n",
    "  axs[1,1].set_title(\"Instantaneous Regret over Time\", y=-0.18)\n",
    "\n",
    "  #plot the total regret over time\n",
    "\n",
    "  total_l_over_t_runs_avgs = np.mean(total_l_over_t_runs, axis=0)\n",
    "  axs[2,0].plot(total_l_over_t_runs_avgs, label = algorithm)\n",
    "\n",
    "  axs[2,0].legend()\n",
    "  axs[2,0].set_xlabel(\"time step\")\n",
    "  axs[2,0].set_ylabel(\"regret\")\n",
    "  axs[2,0].set_title(\"Total Regret up to Time Step t\", y=-0.18)\n",
    "\n",
    "axs[-1, -1].axis('off')\n",
    "\n",
    "title = r'Graphs for Different Algorithms'\n",
    "fig.suptitle(title, fontsize=16, y=0.08)\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wwel6MCoc8ms"
   },
   "source": [
    "### Answers\n",
    "We chose the best hyperparameter for each algorithm by comparing the average total reward at the end of 1000 timesteps. The best algorithm is epsilon = 0 since our arms are stationary and we have a small standard deviation. Then gradient bandit with alpha-decay (alpha = 0.5, lambda_=0.1, p=0.5,) performs the second best. Both epsilon and gradient bandit algorithms total logarithmic regret lines, while thompson seems to be linear (which is not expected).\n",
    "When epsilon-greedy finds the best value it sticky with it, however, Gradient Bandit still keeps exploring even though it knows the best action. This behavior explains the fluctuaions in instantaneous regret and reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOWhRBqJP1nP"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFBJd8hRHDbj"
   },
   "source": "## Q9 Non-stationary Environment\n"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "id": "PbNLZxmiSLnP",
    "outputId": "eb55c51e-58f5-4f01-a714-ea1f8dc3dc66",
    "ExecuteTime": {
     "end_time": "2025-01-30T19:41:18.095966Z",
     "start_time": "2025-01-30T19:41:10.528976Z"
    }
   },
   "source": [
    "delta = 0.2\n",
    "new_mean = [0.5,0.5+2*delta,0.5-2*delta]\n",
    "\n",
    "algorithm_list = [\"epsilon\", \"gradient\", \"thompson\"]\n",
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(18, 18))\n",
    "\n",
    "for algorithm in algorithm_list:\n",
    "\n",
    "  # arrays of the data generated from 100 runs\n",
    "  R_over_t_runs = []\n",
    "  total_R_over_t_runs = []\n",
    "  est_is_best_over_t_runs = []\n",
    "  l_over_t_runs = []\n",
    "  total_l_over_t_runs = []\n",
    "\n",
    "  for run in range(100):\n",
    "    R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = choose_algorithms(algorithm, three_arm_gaussian_bandit, num_time_step=1000, stationary=False, change_time_step=500, bandit_new_means=new_mean)\n",
    "\n",
    "    R_over_t_runs.append(R_over_t)\n",
    "    total_R_over_t_runs.append(total_R_over_t)\n",
    "    est_is_best_over_t_runs.append(est_is_best_over_t)\n",
    "    l_over_t_runs.append(l_over_t)\n",
    "    total_l_over_t_runs.append(total_l_over_t)\n",
    "\n",
    "  R_over_t_runs = np.asarray(R_over_t_runs)\n",
    "  total_R_over_t_runs = np.asarray(total_R_over_t_runs)\n",
    "  # est_is_best_over_t_runs = np.asarray(est_is_best_over_t_runs)\n",
    "  # l_over_t_runs = np.asarray(l_over_t_runs)\n",
    "  # total_l_over_t_runs = np.asarray(total_l_over_t_runs)\n",
    "\n",
    "  # plot the mean reward over time\n",
    "\n",
    "  mean_R_over_t_runs = np.mean(R_over_t_runs, axis=0)\n",
    "  # print(mean_R_over_t_runs)\n",
    "  std_err_R_over_t_runs = np.std(R_over_t_runs, axis=0) / np.sqrt(np.size(R_over_t_runs, axis=0))\n",
    "\n",
    "  axs[0,0].plot(mean_R_over_t_runs, label = algorithm)\n",
    "\n",
    "  R_over_t_minus_std_err = mean_R_over_t_runs - std_err_R_over_t_runs\n",
    "  R_over_t_plus_std_err = mean_R_over_t_runs  + std_err_R_over_t_runs\n",
    "  axs[0,0].fill_between(range(0,1000), R_over_t_minus_std_err, R_over_t_plus_std_err, alpha=0.4)\n",
    "  # axs[0,0].errorbar(range(0,1000), mean_R_over_t_runs, yerr=std_err_R_over_t_runs)\n",
    "\n",
    "  axs[0,0].legend()\n",
    "  axs[0,0].set_xlabel(\"time step\")\n",
    "  axs[0,0].set_ylabel(\"reward value\")\n",
    "  axs[0,0].set_title(\"Average Instantaneous Reward Received over Time\", y=-0.18)\n",
    "\n",
    "  # plot the mean cummulative reward over time\n",
    "\n",
    "  mean_total_R_over_t_runs = np.mean(total_R_over_t_runs, axis=0) # different alphas seem to be performing exactly the same\n",
    "  std_err_total_R_over_t_runs = np.std(total_R_over_t_runs, axis=0) / np.sqrt(np.size(total_R_over_t_runs, axis=0))\n",
    "\n",
    "  axs[0,1].plot(mean_total_R_over_t_runs, label = algorithm)\n",
    "\n",
    "  total_R_over_t_minus_std_err = mean_total_R_over_t_runs - std_err_total_R_over_t_runs\n",
    "  total_R_over_t_plus_std_err = mean_total_R_over_t_runs  + std_err_total_R_over_t_runs\n",
    "  axs[0,1].fill_between(range(0,1000), total_R_over_t_minus_std_err, total_R_over_t_plus_std_err, alpha=0.4)\n",
    "\n",
    "  axs[0,1].legend()\n",
    "  axs[0,1].set_xlabel(\"time step\")\n",
    "  axs[0,1].set_ylabel(\"reward value\")\n",
    "  axs[0,1].set_title(\"Average Cumulative Reward Received over Time\", y=-0.18)\n",
    "\n",
    "axs[-1, -1].axis('off')\n",
    "\n",
    "title = r'Graphs for Gradient bandit with different alphas and alpha decay'\n",
    "fig.suptitle(title, fontsize=16, y=0.08)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "delta = 0.2\n",
    "new_mean = [0.5,0.5+2*delta,0.5-2*delta]\n",
    "\n",
    "algorithm_list = [\"epsilon\", \"gradient\", \"thompson\"]\n",
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(18, 18))\n",
    "\n",
    "for algorithm in algorithm_list:\n",
    "\n",
    "  # arrays of the data generated from 100 runs\n",
    "  R_over_t_runs = []\n",
    "  total_R_over_t_runs = []\n",
    "  est_is_best_over_t_runs = []\n",
    "  l_over_t_runs = []\n",
    "  total_l_over_t_runs = []\n",
    "\n",
    "  for run in range(100):\n",
    "    R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = choose_algorithms(algorithm, three_arm_gaussian_bandit, num_time_step=1000, stationary=True, change_time_step=500, bandit_new_means=new_mean)\n",
    "\n",
    "    R_over_t_runs.append(R_over_t)\n",
    "    total_R_over_t_runs.append(total_R_over_t)\n",
    "    est_is_best_over_t_runs.append(est_is_best_over_t)\n",
    "    l_over_t_runs.append(l_over_t)\n",
    "    total_l_over_t_runs.append(total_l_over_t)\n",
    "\n",
    "  R_over_t_runs = np.asarray(R_over_t_runs)\n",
    "  total_R_over_t_runs = np.asarray(total_R_over_t_runs)\n",
    "  # est_is_best_over_t_runs = np.asarray(est_is_best_over_t_runs)\n",
    "  # l_over_t_runs = np.asarray(l_over_t_runs)\n",
    "  # total_l_over_t_runs = np.asarray(total_l_over_t_runs)\n",
    "\n",
    "  # plot the mean reward over time\n",
    "\n",
    "  mean_R_over_t_runs = np.mean(R_over_t_runs, axis=0)\n",
    "  # print(mean_R_over_t_runs)\n",
    "  std_err_R_over_t_runs = np.std(R_over_t_runs, axis=0) / np.sqrt(np.size(R_over_t_runs, axis=0))\n",
    "\n",
    "  axs[0,0].plot(mean_R_over_t_runs, label = algorithm)\n",
    "\n",
    "  R_over_t_minus_std_err = mean_R_over_t_runs - std_err_R_over_t_runs\n",
    "  R_over_t_plus_std_err = mean_R_over_t_runs  + std_err_R_over_t_runs\n",
    "  axs[0,0].fill_between(range(0,1000), R_over_t_minus_std_err, R_over_t_plus_std_err, alpha=0.4)\n",
    "  # axs[0,0].errorbar(range(0,1000), mean_R_over_t_runs, yerr=std_err_R_over_t_runs)\n",
    "\n",
    "  axs[0,0].legend()\n",
    "  axs[0,0].set_xlabel(\"time step\")\n",
    "  axs[0,0].set_ylabel(\"reward value\")\n",
    "  axs[0,0].set_title(\"Average Instantaneous Reward Received over Time\", y=-0.18)\n",
    "\n",
    "  # plot the mean cummulative reward over time\n",
    "\n",
    "  mean_total_R_over_t_runs = np.mean(total_R_over_t_runs, axis=0) # different alphas seem to be performing exactly the same\n",
    "  std_err_total_R_over_t_runs = np.std(total_R_over_t_runs, axis=0) / np.sqrt(np.size(total_R_over_t_runs, axis=0))\n",
    "\n",
    "  axs[0,1].plot(mean_total_R_over_t_runs, label = algorithm)\n",
    "\n",
    "  total_R_over_t_minus_std_err = mean_total_R_over_t_runs - std_err_total_R_over_t_runs\n",
    "  total_R_over_t_plus_std_err = mean_total_R_over_t_runs  + std_err_total_R_over_t_runs\n",
    "  axs[0,1].fill_between(range(0,1000), total_R_over_t_minus_std_err, total_R_over_t_plus_std_err, alpha=0.4)\n",
    "\n",
    "  axs[0,1].legend()\n",
    "  axs[0,1].set_xlabel(\"time step\")\n",
    "  axs[0,1].set_ylabel(\"reward value\")\n",
    "  axs[0,1].set_title(\"Average Cumulative Reward Received over Time\", y=-0.18)\n",
    "\n",
    "  # #plot the mean percentage of the estimated best action being the third action\n",
    "  # \n",
    "  # est_is_best_over_t_runs_avgs = np.mean(est_is_best_over_t_runs, axis=0)\n",
    "  # plt_est_is_best_over_t_runs_avgs, = axs[1,0].plot(est_is_best_over_t_runs_avgs,label = algorithm)\n",
    "  # \n",
    "  # axs[1,0].legend()\n",
    "  # axs[1,0].set_xlabel(\"time step\")\n",
    "  # axs[1,0].set_ylabel(\"percentage\")\n",
    "  # axs[1,0].set_title(\"Percentage of Runs where Best Action was Chosen\", y=-0.18)\n",
    "  # \n",
    "  # #plot the mean instantaneous regret over time\n",
    "  # \n",
    "  # l_over_t_runs_avgs = np.mean(l_over_t_runs, axis=0)\n",
    "  # axs[1,1].plot(l_over_t_runs_avgs, label = algorithm)\n",
    "  # \n",
    "  # axs[1,1].legend()\n",
    "  # axs[1,1].set_xlabel(\"time step\")\n",
    "  # axs[1,1].set_ylabel(\"regret\")\n",
    "  # axs[1,1].set_title(\"Instantaneous Regret over Time\", y=-0.18)\n",
    "  # \n",
    "  # #plot the total regret over time\n",
    "  # \n",
    "  # total_l_over_t_runs_avgs = np.mean(total_l_over_t_runs, axis=0)\n",
    "  # axs[2,0].plot(total_l_over_t_runs_avgs, label = algorithm)\n",
    "  # \n",
    "  # axs[2,0].legend()\n",
    "  # axs[2,0].set_xlabel(\"time step\")\n",
    "  # axs[2,0].set_ylabel(\"regret\")\n",
    "  # axs[2,0].set_title(\"Total Regret up to Time Step t\", y=-0.18)\n",
    "\n",
    "axs[-1, -1].axis('off')\n",
    "\n",
    "title = r'Graphs for Different Algorithms with Non-stationary Environment'\n",
    "fig.suptitle(title, fontsize=16, y=0.08)\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQ5aVJBqc-8j"
   },
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fe6BJtfS8XIq"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
